{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3e7a6bf",
   "metadata": {},
   "source": [
    "# Agentic Framework — Fully Automatic Google Colab Deployment\n",
    "\n",
    "**One-click deployment**: Just click **Runtime → Run all** (or `Ctrl+F9`) and everything will start automatically.\n",
    "\n",
    "### What this does\n",
    "1. Verifies GPU (H100/A100) and system resources\n",
    "2. Installs system dependencies (PostgreSQL, Redis, Node.js 22, MinIO)\n",
    "3. Installs Ollama + pulls DeepSeek R1 14B (GPU-accelerated)\n",
    "4. Clones the repo and installs Python packages\n",
    "5. Starts all infrastructure (PostgreSQL, Redis, ChromaDB, MinIO)\n",
    "6. Starts all 5 microservices + dashboard\n",
    "7. Creates ngrok tunnels for external access\n",
    "8. Runs health checks\n",
    "9. Keeps the session alive so Colab doesn't disconnect\n",
    "\n",
    "### Prerequisites\n",
    "- Google Colab **Pro** account (for GPU access)\n",
    "- Runtime set to **GPU** (Runtime → Change runtime type → T4/A100/H100)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c70cab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║  CONFIGURATION — Edit these before running                  ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "\n",
    "# GitHub repo to clone\n",
    "REPO_URL = \"https://github.com/landonking-gif/ai_final.git\"\n",
    "\n",
    "# (Optional) Set your ngrok auth token for stable URLs\n",
    "# Get one free at https://dashboard.ngrok.com/signup\n",
    "NGROK_AUTH_TOKEN = \"\"  # Leave empty to skip\n",
    "\n",
    "# LLM model to use (pulled via Ollama)\n",
    "PRIMARY_MODEL = \"deepseek-r1:14b\"\n",
    "FALLBACK_MODEL = \"llama3.2:3b\"\n",
    "\n",
    "# Whether to start the React dashboard (adds ~30s startup)\n",
    "START_DASHBOARD = True\n",
    "\n",
    "# Whether to create ngrok tunnel for external access\n",
    "ENABLE_NGROK = True\n",
    "\n",
    "print(\"Configuration loaded. Running full deployment...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66186129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║  PHASE 1: System Check & Dependencies                      ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "import subprocess, os, sys, shutil, time\n",
    "\n",
    "def run_cmd(cmd, desc=\"\", check=False):\n",
    "    \"\"\"Run a shell command with status output.\"\"\"\n",
    "    if desc:\n",
    "        print(f\"  [{desc}]\", end=\" \", flush=True)\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    if desc:\n",
    "        print(\"OK\" if result.returncode == 0 else f\"WARN ({result.stderr[:120]})\")\n",
    "    if check and result.returncode != 0:\n",
    "        raise RuntimeError(f\"{desc} failed: {result.stderr[:300]}\")\n",
    "    return result\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 1: SYSTEM CHECK & DEPENDENCY INSTALL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- GPU Check ---\n",
    "gpu_check = subprocess.run(\n",
    "    [\"nvidia-smi\", \"--query-gpu=name,memory.total,driver_version\", \"--format=csv,noheader\"],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "if gpu_check.returncode == 0:\n",
    "    print(f\"  [GPU] {gpu_check.stdout.strip()}\")\n",
    "else:\n",
    "    print(\"  [GPU] No GPU detected — LLM inference will be slow on CPU!\")\n",
    "    print(\"         Go to Runtime > Change runtime type > GPU\")\n",
    "\n",
    "# --- RAM & Disk ---\n",
    "try:\n",
    "    import psutil\n",
    "    ram_gb = psutil.virtual_memory().total / (1024**3)\n",
    "    print(f\"  [RAM] {ram_gb:.1f} GB\")\n",
    "except ImportError:\n",
    "    pass\n",
    "disk = shutil.disk_usage(\"/\")\n",
    "print(f\"  [Disk] {disk.free / (1024**3):.1f} GB free\")\n",
    "print(f\"  [Python] {sys.version.split()[0]}\")\n",
    "\n",
    "# --- Install System Dependencies ---\n",
    "print(\"\\n  Installing system packages...\")\n",
    "run_cmd(\"apt-get update -qq 2>/dev/null\", \"apt update\")\n",
    "run_cmd(\"apt-get install -y -qq postgresql postgresql-client redis-server build-essential libpq-dev > /dev/null 2>&1\", \"PostgreSQL + Redis + build tools\")\n",
    "\n",
    "# Node.js 22\n",
    "run_cmd(\"curl -fsSL https://deb.nodesource.com/setup_22.x | bash - > /dev/null 2>&1\", \"Node.js 22 repo\")\n",
    "run_cmd(\"apt-get install -y -qq nodejs > /dev/null 2>&1\", \"Node.js 22\")\n",
    "\n",
    "# MinIO binary\n",
    "run_cmd(\"wget -q https://dl.min.io/server/minio/release/linux-amd64/minio -O /usr/local/bin/minio && chmod +x /usr/local/bin/minio\", \"MinIO\")\n",
    "\n",
    "node_ver = subprocess.run(\"node --version\", shell=True, capture_output=True, text=True)\n",
    "print(f\"  [Node.js] {node_ver.stdout.strip()}\")\n",
    "print(\"\\n  Phase 1 complete.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8960bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║  PHASE 2: Ollama + LLM Models (GPU-Accelerated)            ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "import subprocess, os, time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 2: OLLAMA + LLM MODEL SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Install Ollama\n",
    "print(\"  Installing Ollama...\", end=\" \", flush=True)\n",
    "result = subprocess.run(\"curl -fsSL https://ollama.com/install.sh | sh\",\n",
    "                        shell=True, capture_output=True, text=True)\n",
    "print(\"OK\" if result.returncode == 0 else f\"WARN: {result.stderr[:200]}\")\n",
    "\n",
    "# Start Ollama server in background\n",
    "print(\"  Starting Ollama server...\", end=\" \", flush=True)\n",
    "os.environ[\"OLLAMA_HOST\"] = \"0.0.0.0:11434\"\n",
    "subprocess.Popen(\n",
    "    [\"ollama\", \"serve\"],\n",
    "    stdout=open(\"/tmp/ollama.log\", \"w\"),\n",
    "    stderr=subprocess.STDOUT,\n",
    "    env={**os.environ, \"OLLAMA_HOST\": \"0.0.0.0:11434\"}\n",
    ")\n",
    "time.sleep(5)\n",
    "print(\"OK\")\n",
    "\n",
    "# Pull primary model\n",
    "print(f\"  Pulling {PRIMARY_MODEL} (this may take 2-8 min)...\")\n",
    "subprocess.run([\"ollama\", \"pull\", PRIMARY_MODEL], capture_output=False, text=True)\n",
    "\n",
    "# Pull fallback model\n",
    "print(f\"  Pulling {FALLBACK_MODEL}...\")\n",
    "subprocess.run([\"ollama\", \"pull\", FALLBACK_MODEL], capture_output=False, text=True)\n",
    "\n",
    "# Verify\n",
    "print(\"\\n  Available models:\")\n",
    "subprocess.run([\"ollama\", \"list\"], capture_output=False, text=True)\n",
    "\n",
    "print(\"\\n  Phase 2 complete.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b9f665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║  PHASE 3: Clone Repo + Install Python Packages             ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "import subprocess, os, sys\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 3: REPO CLONE & PYTHON DEPENDENCIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "INSTALL_DIR = \"/content/ai_final\"\n",
    "FRAMEWORK_DIR = f\"{INSTALL_DIR}/agentic-framework-main\"\n",
    "\n",
    "# Clone or update\n",
    "if os.path.exists(INSTALL_DIR):\n",
    "    print(\"  Repo exists — pulling latest...\")\n",
    "    subprocess.run([\"git\", \"-C\", INSTALL_DIR, \"pull\"], capture_output=False, text=True)\n",
    "else:\n",
    "    print(f\"  Cloning {REPO_URL}...\")\n",
    "    subprocess.run([\"git\", \"clone\", REPO_URL, INSTALL_DIR], capture_output=False, text=True)\n",
    "\n",
    "os.chdir(FRAMEWORK_DIR)\n",
    "\n",
    "# Create symlinks (hyphenated dirs → underscored for Python imports)\n",
    "symlinks = {\n",
    "    \"memory_service\": \"memory-service\",\n",
    "    \"subagent_manager\": \"subagent-manager\",\n",
    "    \"mcp_gateway\": \"mcp-gateway\",\n",
    "    \"code_exec\": \"code-exec\",\n",
    "}\n",
    "for link_name, target in symlinks.items():\n",
    "    if not os.path.exists(link_name) and os.path.exists(target):\n",
    "        os.symlink(target, link_name)\n",
    "        print(f\"  Symlink: {link_name} -> {target}\")\n",
    "\n",
    "# Install Python dependencies\n",
    "print(\"\\n  Installing Python packages (2-3 min)...\")\n",
    "subprocess.run(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "     \"-r\", f\"{FRAMEWORK_DIR}/requirements.txt\"],\n",
    "    capture_output=False, text=True\n",
    ")\n",
    "\n",
    "# Extra packages for Colab\n",
    "subprocess.run(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "     \"pyngrok\", \"asyncpg\", \"aiofiles\", \"psutil\"],\n",
    "    capture_output=False, text=True\n",
    ")\n",
    "\n",
    "# Install OpenClaw\n",
    "print(\"  Installing OpenClaw...\")\n",
    "subprocess.run([\"npm\", \"install\", \"-g\", \"openclaw@latest\"],\n",
    "               capture_output=True, text=True)\n",
    "\n",
    "# Add framework to PYTHONPATH\n",
    "if FRAMEWORK_DIR not in sys.path:\n",
    "    sys.path.insert(0, FRAMEWORK_DIR)\n",
    "os.environ[\"PYTHONPATH\"] = FRAMEWORK_DIR\n",
    "\n",
    "print(f\"\\n  Framework directory: {FRAMEWORK_DIR}\")\n",
    "print(\"  Phase 3 complete.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fdaf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║  PHASE 4: Start Infrastructure + All Services               ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "import subprocess, os, sys, time, urllib.request, json\n",
    "\n",
    "FRAMEWORK_DIR = \"/content/ai_final/agentic-framework-main\"\n",
    "os.chdir(FRAMEWORK_DIR)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 4: INFRASTRUCTURE & SERVICES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ──────────── Infrastructure ────────────\n",
    "print(\"\\n── Infrastructure ──\")\n",
    "\n",
    "# PostgreSQL\n",
    "print(\"  Starting PostgreSQL...\", end=\" \", flush=True)\n",
    "subprocess.run(\"service postgresql start\", shell=True, capture_output=True)\n",
    "time.sleep(2)\n",
    "for cmd in [\n",
    "    \"CREATE USER agent_user WITH PASSWORD 'agent_pass' CREATEDB;\",\n",
    "    \"CREATE DATABASE agentic_framework OWNER agent_user;\",\n",
    "    \"GRANT ALL PRIVILEGES ON DATABASE agentic_framework TO agent_user;\",\n",
    "]:\n",
    "    subprocess.run([\"sudo\", \"-u\", \"postgres\", \"psql\", \"-c\", cmd],\n",
    "                   capture_output=True, text=True)\n",
    "pg = subprocess.run([\"sudo\", \"-u\", \"postgres\", \"psql\", \"-c\", \"SELECT 1;\"],\n",
    "                    capture_output=True, text=True)\n",
    "print(\"OK\" if pg.returncode == 0 else \"FAIL\")\n",
    "\n",
    "# Redis\n",
    "print(\"  Starting Redis...\", end=\" \", flush=True)\n",
    "subprocess.run(\"redis-server --daemonize yes --port 6379\", shell=True, capture_output=True)\n",
    "time.sleep(1)\n",
    "redis_ok = subprocess.run(\"redis-cli ping\", shell=True, capture_output=True, text=True)\n",
    "print(\"OK\" if \"PONG\" in redis_ok.stdout else \"FAIL\")\n",
    "\n",
    "# ChromaDB\n",
    "print(\"  Starting ChromaDB...\", end=\" \", flush=True)\n",
    "os.makedirs(\"/tmp/chroma_data\", exist_ok=True)\n",
    "subprocess.Popen(\n",
    "    [\"chroma\", \"run\", \"--host\", \"0.0.0.0\", \"--port\", \"8001\", \"--path\", \"/tmp/chroma_data\"],\n",
    "    stdout=open(\"/tmp/chroma.log\", \"w\"), stderr=subprocess.STDOUT\n",
    ")\n",
    "time.sleep(3)\n",
    "print(\"OK\")\n",
    "\n",
    "# MinIO\n",
    "print(\"  Starting MinIO...\", end=\" \", flush=True)\n",
    "os.makedirs(\"/tmp/minio_data\", exist_ok=True)\n",
    "subprocess.Popen(\n",
    "    [\"/usr/local/bin/minio\", \"server\", \"/tmp/minio_data\",\n",
    "     \"--address\", \":9000\", \"--console-address\", \":9001\"],\n",
    "    stdout=open(\"/tmp/minio.log\", \"w\"), stderr=subprocess.STDOUT,\n",
    "    env={**os.environ, \"MINIO_ROOT_USER\": \"minioadmin\", \"MINIO_ROOT_PASSWORD\": \"minioadmin\"}\n",
    ")\n",
    "time.sleep(2)\n",
    "print(\"OK\")\n",
    "\n",
    "# ──────────── Environment Variables ────────────\n",
    "env_vars = {\n",
    "    \"POSTGRES_URL\": \"postgresql://agent_user:agent_pass@localhost:5432/agentic_framework\",\n",
    "    \"REDIS_URL\": \"redis://localhost:6379/0\",\n",
    "    \"MCP_GATEWAY_URL\": \"http://localhost:8080\",\n",
    "    \"MEMORY_SERVICE_URL\": \"http://localhost:8002\",\n",
    "    \"SUBAGENT_MANAGER_URL\": \"http://localhost:8003\",\n",
    "    \"CODE_EXECUTOR_URL\": \"http://localhost:8004\",\n",
    "    \"OLLAMA_ENDPOINT\": \"http://localhost:11434\",\n",
    "    \"OLLAMA_BASE_URL\": \"http://localhost:11434\",\n",
    "    \"LOCAL_MODEL\": PRIMARY_MODEL,\n",
    "    \"FALLBACK_MODEL\": FALLBACK_MODEL,\n",
    "    \"DEFAULT_LLM_PROVIDER\": \"ollama\",\n",
    "    \"LLM_PROVIDER\": \"ollama\",\n",
    "    \"USE_OPENCLAW\": \"false\",\n",
    "    \"CHROMA_URL\": \"http://localhost:8001\",\n",
    "    \"MINIO_ENDPOINT\": \"localhost:9000\",\n",
    "    \"MINIO_ACCESS_KEY\": \"minioadmin\",\n",
    "    \"MINIO_SECRET_KEY\": \"minioadmin\",\n",
    "    \"JWT_SECRET_KEY\": \"colab-dev-secret-key-change-in-production\",\n",
    "    \"ENVIRONMENT\": \"development\",\n",
    "    \"PYTHONPATH\": FRAMEWORK_DIR,\n",
    "    \"WORKSPACE_ROOT\": f\"{FRAMEWORK_DIR}/workspace\",\n",
    "    \"WEBSOCKET_ENABLED\": \"true\",\n",
    "    \"INDEX_CODEBASE\": \"true\",\n",
    "}\n",
    "for k, v in env_vars.items():\n",
    "    os.environ[k] = v\n",
    "\n",
    "with open(f\"{FRAMEWORK_DIR}/.env\", \"w\") as f:\n",
    "    for k, v in env_vars.items():\n",
    "        f.write(f\"{k}={v}\\n\")\n",
    "\n",
    "# Create workspace dirs\n",
    "for d in [\"workspace/.copilot/memory/diary\", \"workspace/.copilot/memory/reflections\", \"workspace/ralph-work\"]:\n",
    "    os.makedirs(f\"{FRAMEWORK_DIR}/{d}\", exist_ok=True)\n",
    "\n",
    "print(\"  Environment configured.\")\n",
    "\n",
    "# ──────────── Start Microservices ────────────\n",
    "print(\"\\n── Microservices ──\")\n",
    "\n",
    "service_env = {**os.environ}\n",
    "\n",
    "services = [\n",
    "    {\n",
    "        \"name\": \"MCP Gateway\",\n",
    "        \"module\": \"mcp_gateway.service.main:app\",\n",
    "        \"port\": 8080,\n",
    "        \"log\": \"/tmp/mcp_gateway.log\",\n",
    "        \"env\": {\"REDIS_URL\": \"redis://localhost:6379/3\"},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Memory Service\",\n",
    "        \"module\": \"memory_service.service.main:app\",\n",
    "        \"port\": 8002,\n",
    "        \"log\": \"/tmp/memory_service.log\",\n",
    "        \"env\": {\"REDIS_URL\": \"redis://localhost:6379/2\"},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"SubAgent Manager\",\n",
    "        \"module\": \"subagent_manager.service.main:app\",\n",
    "        \"port\": 8003,\n",
    "        \"log\": \"/tmp/subagent_manager.log\",\n",
    "        \"env\": {\n",
    "            \"REDIS_URL\": \"redis://localhost:6379/1\",\n",
    "            \"SUBAGENT_USE_OPENCLAW\": \"false\",\n",
    "            \"SUBAGENT_LLM_PROVIDER\": \"ollama\",\n",
    "            \"SUBAGENT_LLM_MODEL\": PRIMARY_MODEL,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Code Executor\",\n",
    "        \"module\": \"code_exec.service.main:app\",\n",
    "        \"port\": 8004,\n",
    "        \"log\": \"/tmp/code_exec.log\",\n",
    "        \"env\": {\"REDIS_URL\": \"redis://localhost:6379/4\"},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Orchestrator\",\n",
    "        \"module\": \"orchestrator.service.main:app\",\n",
    "        \"port\": 8000,\n",
    "        \"log\": \"/tmp/orchestrator.log\",\n",
    "        \"env\": {},\n",
    "    },\n",
    "]\n",
    "\n",
    "started = {}\n",
    "for svc in services:\n",
    "    print(f\"  Starting {svc['name']} (:{svc['port']})...\", end=\" \", flush=True)\n",
    "    svc_env = {**service_env, **svc[\"env\"]}\n",
    "    proc = subprocess.Popen(\n",
    "        [sys.executable, \"-m\", \"uvicorn\", svc[\"module\"],\n",
    "         \"--host\", \"0.0.0.0\", \"--port\", str(svc[\"port\"])],\n",
    "        cwd=FRAMEWORK_DIR,\n",
    "        stdout=open(svc[\"log\"], \"w\"),\n",
    "        stderr=subprocess.STDOUT,\n",
    "        env=svc_env\n",
    "    )\n",
    "    started[svc[\"name\"]] = proc.pid\n",
    "    time.sleep(3)\n",
    "    print(f\"OK (PID {proc.pid})\")\n",
    "\n",
    "# ──────────── Dashboard ────────────\n",
    "if START_DASHBOARD:\n",
    "    print(\"\\n── Dashboard ──\")\n",
    "    dashboard_dir = f\"{FRAMEWORK_DIR}/dashboard\"\n",
    "    if os.path.exists(f\"{dashboard_dir}/build\"):\n",
    "        # Serve the pre-built dashboard\n",
    "        print(\"  Serving pre-built dashboard (port 3000)...\", end=\" \", flush=True)\n",
    "        subprocess.Popen(\n",
    "            [\"npx\", \"serve\", \"-s\", \"build\", \"-l\", \"3000\"],\n",
    "            cwd=dashboard_dir,\n",
    "            stdout=open(\"/tmp/dashboard.log\", \"w\"),\n",
    "            stderr=subprocess.STDOUT,\n",
    "            env={**os.environ, \"PORT\": \"3000\"}\n",
    "        )\n",
    "        time.sleep(3)\n",
    "        print(\"OK\")\n",
    "    elif os.path.exists(f\"{dashboard_dir}/package.json\"):\n",
    "        print(\"  Installing dashboard deps & starting (port 3000)...\", end=\" \", flush=True)\n",
    "        subprocess.run([\"npm\", \"install\"], cwd=dashboard_dir, capture_output=True)\n",
    "        subprocess.Popen(\n",
    "            [\"npm\", \"start\"],\n",
    "            cwd=dashboard_dir,\n",
    "            stdout=open(\"/tmp/dashboard.log\", \"w\"),\n",
    "            stderr=subprocess.STDOUT,\n",
    "            env={**os.environ, \"PORT\": \"3000\", \"BROWSER\": \"none\"}\n",
    "        )\n",
    "        time.sleep(5)\n",
    "        print(\"OK\")\n",
    "    else:\n",
    "        print(\"  Dashboard not found — skipping.\")\n",
    "\n",
    "# ──────────── Wait & Health Check ────────────\n",
    "print(\"\\n  Waiting 15s for services to initialize...\")\n",
    "time.sleep(15)\n",
    "\n",
    "print(\"\\n── Health Checks ──\")\n",
    "endpoints = [\n",
    "    (\"Orchestrator\",    \"http://localhost:8000/health\"),\n",
    "    (\"Memory Service\",  \"http://localhost:8002/health\"),\n",
    "    (\"SubAgent Manager\",\"http://localhost:8003/health\"),\n",
    "    (\"MCP Gateway\",     \"http://localhost:8080/health\"),\n",
    "    (\"Code Executor\",   \"http://localhost:8004/health\"),\n",
    "    (\"Ollama\",          \"http://localhost:11434/api/tags\"),\n",
    "]\n",
    "\n",
    "all_ok = True\n",
    "for name, url in endpoints:\n",
    "    try:\n",
    "        req = urllib.request.urlopen(url, timeout=5)\n",
    "        print(f\"  {name:20s} : OK ({req.getcode()})\")\n",
    "    except Exception as e:\n",
    "        all_ok = False\n",
    "        print(f\"  {name:20s} : STARTING ({str(e)[:50]})\")\n",
    "\n",
    "if all_ok:\n",
    "    print(\"\\n  ALL SERVICES HEALTHY\")\n",
    "else:\n",
    "    print(\"\\n  Some services still starting. They should be ready in ~30s.\")\n",
    "\n",
    "print(\"\\n  Phase 4 complete.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b7a06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║  PHASE 5: External Access (ngrok Tunnels)                   ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "import os\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 5: EXTERNAL ACCESS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "api_url = \"http://localhost:8000\"  # default fallback\n",
    "dashboard_url = \"http://localhost:3000\"\n",
    "\n",
    "if ENABLE_NGROK:\n",
    "    from pyngrok import ngrok, conf\n",
    "\n",
    "    if NGROK_AUTH_TOKEN:\n",
    "        ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "        print(\"  ngrok auth token set (stable URLs enabled)\")\n",
    "\n",
    "    # API tunnel\n",
    "    print(\"  Creating tunnel for Orchestrator API (port 8000)...\")\n",
    "    api_tunnel = ngrok.connect(8000, \"http\")\n",
    "    api_url = api_tunnel.public_url\n",
    "\n",
    "    # Dashboard tunnel (if running)\n",
    "    if START_DASHBOARD:\n",
    "        print(\"  Creating tunnel for Dashboard (port 3000)...\")\n",
    "        dash_tunnel = ngrok.connect(3000, \"http\")\n",
    "        dashboard_url = dash_tunnel.public_url\n",
    "\n",
    "    os.environ[\"COLAB_API_URL\"] = api_url\n",
    "    os.environ[\"COLAB_DASHBOARD_URL\"] = dashboard_url\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"╔══════════════════════════════════════════════════════════╗\")\n",
    "    print(\"║  PUBLIC ACCESS URLS (share these!)                      ║\")\n",
    "    print(\"╠══════════════════════════════════════════════════════════╣\")\n",
    "    print(f\"║  API:        {api_url:<43s}║\")\n",
    "    print(f\"║  API Docs:   {api_url + '/docs':<43s}║\")\n",
    "    print(f\"║  Health:     {api_url + '/health':<43s}║\")\n",
    "    print(f\"║  WebSocket:  {api_url.replace('http', 'ws') + '/ws':<43s}║\")\n",
    "    if START_DASHBOARD:\n",
    "        print(f\"║  Dashboard:  {dashboard_url:<43s}║\")\n",
    "    print(\"╚══════════════════════════════════════════════════════════╝\")\n",
    "else:\n",
    "    print(\"  ngrok disabled. Services available at localhost only:\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"  Local endpoints (inside Colab):\")\n",
    "print(\"    Orchestrator:    http://localhost:8000\")\n",
    "print(\"    Memory Service:  http://localhost:8002\")\n",
    "print(\"    SubAgent Mgr:    http://localhost:8003\")\n",
    "print(\"    MCP Gateway:     http://localhost:8080\")\n",
    "print(\"    Code Executor:   http://localhost:8004\")\n",
    "print(\"    Ollama LLM:      http://localhost:11434\")\n",
    "if START_DASHBOARD:\n",
    "    print(\"    Dashboard:       http://localhost:3000\")\n",
    "\n",
    "print(\"\\n  Phase 5 complete.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b927a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║  PHASE 6: Quick Smoke Test                                  ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "import json, urllib.request, time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 6: SMOKE TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "passed = 0\n",
    "total = 0\n",
    "\n",
    "def check(name, url):\n",
    "    global passed, total\n",
    "    total += 1\n",
    "    try:\n",
    "        r = urllib.request.urlopen(url, timeout=10)\n",
    "        passed += 1\n",
    "        print(f\"  [PASS] {name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [FAIL] {name} — {str(e)[:60]}\")\n",
    "\n",
    "check(\"Orchestrator API\",   \"http://localhost:8000/health\")\n",
    "check(\"Memory Service\",     \"http://localhost:8002/health\")\n",
    "check(\"SubAgent Manager\",   \"http://localhost:8003/health\")\n",
    "check(\"MCP Gateway\",        \"http://localhost:8080/health\")\n",
    "check(\"Code Executor\",      \"http://localhost:8004/health\")\n",
    "check(\"Ollama LLM\",         \"http://localhost:11434/api/tags\")\n",
    "\n",
    "# Test LLM inference\n",
    "total += 1\n",
    "print(\"\\n  Testing LLM inference (GPU)...\", end=\" \", flush=True)\n",
    "try:\n",
    "    t0 = time.time()\n",
    "    data = json.dumps({\n",
    "        \"model\": PRIMARY_MODEL,\n",
    "        \"prompt\": \"What is 2+2? Answer in one word.\",\n",
    "        \"stream\": False\n",
    "    }).encode()\n",
    "    req = urllib.request.Request(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        data=data,\n",
    "        headers={\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "    resp = urllib.request.urlopen(req, timeout=120)\n",
    "    result = json.loads(resp.read().decode())\n",
    "    elapsed = time.time() - t0\n",
    "    passed += 1\n",
    "    print(f\"OK ({elapsed:.1f}s)\")\n",
    "    print(f\"    Response: {result.get('response', '???')[:100]}\")\n",
    "except Exception as e:\n",
    "    print(f\"FAIL — {str(e)[:80]}\")\n",
    "\n",
    "print(f\"\\n  Results: {passed}/{total} passed\")\n",
    "if passed == total:\n",
    "    print(\"  ALL SYSTEMS GO!\")\n",
    "else:\n",
    "    print(\"  Some services may still be initializing. Wait 30s and re-run this cell.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737c2ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║  PHASE 7: Keep-Alive (prevents Colab from disconnecting)    ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "#\n",
    "# This cell runs a background loop that:\n",
    "#  1. Pings all services every 60 seconds\n",
    "#  2. Auto-restarts any crashed service\n",
    "#  3. Prints a status update every 5 minutes\n",
    "#  4. Keeps the Colab runtime alive\n",
    "#\n",
    "# Stop it with: Runtime > Interrupt execution (or Ctrl+M I)\n",
    "#\n",
    "import subprocess, os, sys, time, urllib.request, json, signal\n",
    "from datetime import datetime\n",
    "\n",
    "FRAMEWORK_DIR = \"/content/ai_final/agentic-framework-main\"\n",
    "\n",
    "service_defs = [\n",
    "    {\"name\": \"MCP Gateway\",      \"module\": \"mcp_gateway.service.main:app\",      \"port\": 8080, \"log\": \"/tmp/mcp_gateway.log\",      \"env\": {\"REDIS_URL\": \"redis://localhost:6379/3\"}},\n",
    "    {\"name\": \"Memory Service\",   \"module\": \"memory_service.service.main:app\",   \"port\": 8002, \"log\": \"/tmp/memory_service.log\",   \"env\": {\"REDIS_URL\": \"redis://localhost:6379/2\"}},\n",
    "    {\"name\": \"SubAgent Manager\", \"module\": \"subagent_manager.service.main:app\", \"port\": 8003, \"log\": \"/tmp/subagent_manager.log\", \"env\": {\"REDIS_URL\": \"redis://localhost:6379/1\"}},\n",
    "    {\"name\": \"Code Executor\",    \"module\": \"code_exec.service.main:app\",        \"port\": 8004, \"log\": \"/tmp/code_exec.log\",        \"env\": {\"REDIS_URL\": \"redis://localhost:6379/4\"}},\n",
    "    {\"name\": \"Orchestrator\",     \"module\": \"orchestrator.service.main:app\",     \"port\": 8000, \"log\": \"/tmp/orchestrator.log\",     \"env\": {}},\n",
    "]\n",
    "\n",
    "def is_service_alive(port):\n",
    "    try:\n",
    "        url = f\"http://localhost:{port}/health\" if port != 11434 else f\"http://localhost:{port}/api/tags\"\n",
    "        urllib.request.urlopen(url, timeout=5)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def restart_service(svc):\n",
    "    \"\"\"Restart a crashed service.\"\"\"\n",
    "    print(f\"    Restarting {svc['name']} on port {svc['port']}...\", end=\" \", flush=True)\n",
    "    svc_env = {**os.environ, **svc[\"env\"]}\n",
    "    proc = subprocess.Popen(\n",
    "        [sys.executable, \"-m\", \"uvicorn\", svc[\"module\"],\n",
    "         \"--host\", \"0.0.0.0\", \"--port\", str(svc[\"port\"])],\n",
    "        cwd=FRAMEWORK_DIR,\n",
    "        stdout=open(svc[\"log\"], \"a\"),\n",
    "        stderr=subprocess.STDOUT,\n",
    "        env=svc_env\n",
    "    )\n",
    "    time.sleep(5)\n",
    "    print(f\"PID {proc.pid}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"KEEP-ALIVE WATCHDOG STARTED\")\n",
    "print(\"  Monitoring services every 60s with auto-restart.\")\n",
    "print(\"  Status updates every 5 minutes.\")\n",
    "print(\"  Stop with: Runtime > Interrupt execution\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cycle = 0\n",
    "try:\n",
    "    while True:\n",
    "        cycle += 1\n",
    "        restarts = 0\n",
    "\n",
    "        # Check & auto-restart services\n",
    "        for svc in service_defs:\n",
    "            if not is_service_alive(svc[\"port\"]):\n",
    "                restart_service(svc)\n",
    "                restarts += 1\n",
    "\n",
    "        # Check Ollama\n",
    "        if not is_service_alive(11434):\n",
    "            print(\"    Restarting Ollama...\", end=\" \", flush=True)\n",
    "            subprocess.Popen(\n",
    "                [\"ollama\", \"serve\"],\n",
    "                stdout=open(\"/tmp/ollama.log\", \"a\"),\n",
    "                stderr=subprocess.STDOUT,\n",
    "                env={**os.environ, \"OLLAMA_HOST\": \"0.0.0.0:11434\"}\n",
    "            )\n",
    "            time.sleep(5)\n",
    "            print(\"OK\")\n",
    "            restarts += 1\n",
    "\n",
    "        # Status update every 5 minutes (every 5th cycle)\n",
    "        if cycle % 5 == 0:\n",
    "            now = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            alive = sum(1 for s in service_defs if is_service_alive(s[\"port\"]))\n",
    "            ollama_ok = is_service_alive(11434)\n",
    "            print(f\"  [{now}] Services: {alive}/{len(service_defs)} | Ollama: {'OK' if ollama_ok else 'DOWN'} | Restarts this cycle: {restarts}\")\n",
    "\n",
    "        time.sleep(60)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n  Watchdog stopped by user.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44117a36",
   "metadata": {},
   "source": [
    "---\n",
    "## Utility Cells (run manually as needed)\n",
    "\n",
    "The cells below are optional — run them when you want to interact with the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8eb3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Send a task to the Orchestrator ──\n",
    "import json, urllib.request\n",
    "\n",
    "task = \"Write a Python function that calculates the Fibonacci sequence up to n terms, with proper error handling and type hints.\"\n",
    "\n",
    "print(f\"Task: {task}\\n\")\n",
    "data = json.dumps({\"message\": task, \"session_id\": \"colab-auto-001\"}).encode()\n",
    "req = urllib.request.Request(\n",
    "    \"http://localhost:8000/chat\",\n",
    "    data=data,\n",
    "    headers={\"Content-Type\": \"application/json\"}\n",
    ")\n",
    "try:\n",
    "    resp = urllib.request.urlopen(req, timeout=300)\n",
    "    result = json.loads(resp.read().decode())\n",
    "    print(json.dumps(result, indent=2)[:3000])\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Tip: !tail -100 /tmp/orchestrator.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fe6036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── View service logs ──\n",
    "# Change SERVICE to: orchestrator, memory_service, subagent_manager,\n",
    "#                     mcp_gateway, code_exec, ollama, chroma, minio, dashboard\n",
    "SERVICE = \"orchestrator\"\n",
    "LINES = 50\n",
    "\n",
    "import subprocess\n",
    "print(f\"Last {LINES} lines of {SERVICE}:\")\n",
    "print(\"=\" * 60)\n",
    "subprocess.run([\"tail\", f\"-{LINES}\", f\"/tmp/{SERVICE}.log\"], capture_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e3d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── System resource monitor ──\n",
    "import subprocess, psutil, shutil\n",
    "\n",
    "print(\"GPU:\")\n",
    "subprocess.run(\"nvidia-smi\", shell=True)\n",
    "\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"\\nRAM: {mem.used/1024**3:.1f}/{mem.total/1024**3:.1f} GB ({mem.percent}%)\")\n",
    "\n",
    "disk = shutil.disk_usage(\"/\")\n",
    "print(f\"Disk: {(disk.total-disk.free)/1024**3:.1f}/{disk.total/1024**3:.1f} GB\")\n",
    "\n",
    "print(\"\\nRunning services:\")\n",
    "for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n",
    "    try:\n",
    "        cmd = \" \".join(proc.info.get('cmdline', []))\n",
    "        if 'uvicorn' in cmd or 'ollama' in proc.info.get('name', '').lower():\n",
    "            print(f\"  PID {proc.info['pid']}: {cmd[:80]}\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c48e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Restart all services ──\n",
    "import psutil, time\n",
    "\n",
    "print(\"Stopping all services...\")\n",
    "for proc in psutil.process_iter(['pid', 'cmdline']):\n",
    "    try:\n",
    "        cmd = \" \".join(proc.info.get('cmdline', []))\n",
    "        if 'uvicorn' in cmd and 'service.main' in cmd:\n",
    "            proc.kill()\n",
    "            print(f\"  Killed PID {proc.info['pid']}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "time.sleep(3)\n",
    "print(\"Done. Re-run Phase 4 cell to restart services.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
