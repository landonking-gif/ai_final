{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e3e7a6bf",
      "metadata": {
        "id": "e3e7a6bf"
      },
      "source": [
        "# Agentic Framework — Fully Automatic Google Colab Deployment\n",
        "\n",
        "**One-click deployment**: Just click **Runtime → Run all** (or `Ctrl+F9`) and everything will start automatically.\n",
        "\n",
        "### What this does\n",
        "1. Verifies GPU (H100/A100) and system resources\n",
        "2. Installs system dependencies (PostgreSQL, Redis, Node.js 22, MinIO)\n",
        "3. Installs Ollama + pulls DeepSeek R1 14B (GPU-accelerated)\n",
        "4. Clones the repo and installs Python packages\n",
        "5. Starts all infrastructure (PostgreSQL, Redis, ChromaDB, MinIO)\n",
        "6. Starts all 5 microservices + dashboard\n",
        "7. Creates ngrok tunnels for external access\n",
        "8. Runs health checks\n",
        "9. Keeps the session alive so Colab doesn't disconnect\n",
        "\n",
        "### Prerequisites\n",
        "- Google Colab **Pro** account (for GPU access)\n",
        "- Runtime set to **GPU** (Runtime → Change runtime type → T4/A100/H100)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4c70cab1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c70cab1",
        "outputId": "b7195778-b615-4973-d019-afd3d5b09e9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ai_final'...\n",
            "remote: Enumerating objects: 60777, done.\u001b[K\n",
            "remote: Counting objects: 100% (162/162), done.\u001b[K\n",
            "remote: Compressing objects: 100% (130/130), done.\u001b[K\n",
            "remote: Total 60777 (delta 53), reused 124 (delta 30), pack-reused 60615 (from 1)\u001b[K\n",
            "Receiving objects: 100% (60777/60777), 60.30 MiB | 17.27 MiB/s, done.\n",
            "Resolving deltas: 100% (22791/22791), done.\n",
            "Updating files: 100% (79496/79496), done.\n",
            "total 312\n",
            "drwxr-xr-x 10 root root  4096 Feb  8 02:06 .\n",
            "drwxr-xr-x  1 root root  4096 Feb  8 02:05 ..\n",
            "-rw-r--r--  1 root root  4812 Feb  8 02:06 agentic-framework-deploy-auto.ipynb\n",
            "drwxr-xr-x 17 root root  4096 Feb  8 02:06 agentic-framework-main\n",
            "-rw-r--r--  1 root root  1105 Feb  8 02:06 check_deployment_status.ps1\n",
            "-rw-r--r--  1 root root  8252 Feb  8 02:06 colab_automated_deploy.py\n",
            "-rw-r--r--  1 root root 58478 Feb  8 02:06 colab_auto_run.ipynb\n",
            "-rw-r--r--  1 root root  2771 Feb  8 02:06 colab_critical_diagnostic.py\n",
            "-rw-r--r--  1 root root 34957 Feb  8 02:06 colab_deploy.ipynb\n",
            "-rw-r--r--  1 root root 22119 Feb  8 02:06 colab_deployment.py\n",
            "-rw-r--r--  1 root root  3756 Feb  8 02:06 COLAB_DEPLOYMENT_README.md\n",
            "-rw-r--r--  1 root root  5337 Feb  8 02:06 colab_deploy.ps1\n",
            "-rw-r--r--  1 root root  3501 Feb  8 02:06 colab_diagnostic_cell_emergency.py\n",
            "-rw-r--r--  1 root root  5095 Feb  8 02:06 colab_diagnostics.py\n",
            "-rw-r--r--  1 root root  3293 Feb  8 02:06 colab_recovery_cell.py\n",
            "-rw-r--r--  1 root root  3456 Feb  8 02:06 colab_service_recovery.py\n",
            "-rw-r--r--  1 root root  4586 Feb  8 02:06 colab_shortcuts.ipynb\n",
            "-rw-r--r--  1 root root  3631 Feb  8 02:06 COLAB_TROUBLESHOOTING.md\n",
            "drwxr-xr-x  4 root root  4096 Feb  8 02:06 copilot-memory-plugin\n",
            "-rw-r--r--  1 root root  1449 Feb  8 02:06 dashboard_troubleshoot.py\n",
            "-rw-r--r--  1 root root  3439 Feb  8 02:06 deploy_colab_automated.ps1\n",
            "-rw-r--r--  1 root root 18727 Feb  8 02:06 deploy-colab-automation.ps1\n",
            "-rw-r--r--  1 root root  4597 Feb  8 02:06 deploy-colab-cli.ps1\n",
            "-rw-r--r--  1 root root  1899 Feb  8 02:06 deploy-colab-final.ps1\n",
            "-rw-r--r--  1 root root  1527 Feb  8 02:06 deploy_colab_simple.bat\n",
            "-rw-r--r--  1 root root  7411 Feb  8 02:06 deploy-colab-simple.ps1\n",
            "-rw-r--r--  1 root root  1752 Feb  8 02:06 deploy_colab_simple.ps1\n",
            "-rw-r--r--  1 root root  5088 Feb  8 02:06 DEPLOYMENT_FIXES.md\n",
            "drwxr-xr-x  8 root root  4096 Feb  8 02:06 .git\n",
            "-rw-r--r--  1 root root  1599 Feb  8 02:06 hotfix_deploy.ps1\n",
            "-rw-r--r--  1 root root   388 Feb  8 02:06 king-ai-studio.pem\n",
            "drwxr-xr-x  2 root root  4096 Feb  8 02:06 LeCoder-cgpu-CLI\n",
            "drwxr-xr-x  6 root root  4096 Feb  8 02:06 multi-agent-orchestration-main\n",
            "drwxr-xr-x  2 root root  4096 Feb  8 02:06 __pycache__\n",
            "drwxr-xr-x  4 root root  4096 Feb  8 02:06 ralph\n",
            "drwxr-xr-x  5 root root  4096 Feb  8 02:06 ralph-work\n",
            "-rw-r--r--  1 root root  2306 Feb  8 02:06 simple_deploy.ps1\n",
            "-rw-r--r--  1 root root   474 Feb  8 02:06 Untitled-1.ipynb\n",
            "✅ Latest fixes pulled from GitHub!\n",
            "Configuration loaded. Running full deployment...\n"
          ]
        }
      ],
      "source": [
        "# ╔══════════════════════════════════════════════════════════════╗\n",
        "# ║  CONFIGURATION — Edit these before running                  ║\n",
        "# ╚══════════════════════════════════════════════════════════════╝\n",
        "\n",
        "# GitHub repo to clone\n",
        "REPO_URL = \"https://github.com/landonking-gif/ai_final.git\"\n",
        "# ╔══════════════════════════════════════════════════════════════╗\n",
        "# ║  PULL LATEST FIXES FROM GITHUB                              ║\n",
        "# ╚══════════════════════════════════════════════════════════════╝\n",
        "\n",
        "!cd /content && rm -rf ai_final\n",
        "!git clone https://github.com/landonking-gif/ai_final.git\n",
        "!cd ai_final && ls -la\n",
        "\n",
        "print(\"✅ Latest fixes pulled from GitHub!\")\n",
        "\n",
        "# (Optional) Set your ngrok auth token for stable URLs\n",
        "# Get one free at https://dashboard.ngrok.com/signup\n",
        "NGROK_AUTH_TOKEN = \"39MaIP07IiJMHPNDgd3raMEOL6r_2KyacFVXP68bbxBu9s8E8\"\n",
        "\n",
        "# LLM model to use (pulled via Ollama)\n",
        "PRIMARY_MODEL = \"deepseek-r1:14b\"\n",
        "FALLBACK_MODEL = \"llama3.2:3b\"\n",
        "\n",
        "# Whether to start the React dashboard (adds ~30s startup)\n",
        "START_DASHBOARD = True\n",
        "\n",
        "# Whether to create ngrok tunnel for external access\n",
        "ENABLE_NGROK = True\n",
        "\n",
        "print(\"Configuration loaded. Running full deployment...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "66186129",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66186129",
        "outputId": "4b76e4d2-10dd-4e48-a260-cfdf4cbc5ce0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "PHASE 1: SYSTEM CHECK & DEPENDENCY INSTALL\n",
            "============================================================\n",
            "  [GPU] Tesla T4, 15360 MiB, 550.54.15\n",
            "  [RAM] 12.7 GB\n",
            "  [Disk] 178.7 GB free\n",
            "  [Python] 3.12.12\n",
            "\n",
            "  Installing system packages...\n",
            "  [apt update] OK\n",
            "  [PostgreSQL + Redis + build tools + zstd] OK\n",
            "  [Node.js 22 repo] OK\n",
            "  [Node.js 22] OK\n",
            "  [MinIO] OK\n",
            "  [Node.js] v22.22.0\n",
            "\n",
            "  Phase 1 complete.\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ╔══════════════════════════════════════════════════════════════╗\n",
        "# ║  PHASE 1: System Check & Dependencies                      ║\n",
        "# ╚══════════════════════════════════════════════════════════════╝\n",
        "import subprocess, os, sys, shutil, time\n",
        "\n",
        "def run_cmd(cmd, desc=\"\", check=False):\n",
        "    \"\"\"Run a shell command with status output.\"\"\"\n",
        "    if desc:\n",
        "        print(f\"  [{desc}]\", end=\" \", flush=True)\n",
        "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "    if desc:\n",
        "        print(\"OK\" if result.returncode == 0 else f\"WARN ({result.stderr[:120]})\")\n",
        "    if check and result.returncode != 0:\n",
        "        raise RuntimeError(f\"{desc} failed: {result.stderr[:300]}\")\n",
        "    return result\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PHASE 1: SYSTEM CHECK & DEPENDENCY INSTALL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# --- GPU Check ---\n",
        "gpu_check = subprocess.run(\n",
        "    [\"nvidia-smi\", \"--query-gpu=name,memory.total,driver_version\", \"--format=csv,noheader\"],\n",
        "    capture_output=True, text=True\n",
        ")\n",
        "if gpu_check.returncode == 0:\n",
        "    print(f\"  [GPU] {gpu_check.stdout.strip()}\")\n",
        "else:\n",
        "    print(\"  [GPU] No GPU detected — LLM inference will be slow on CPU!\")\n",
        "    print(\"         Go to Runtime > Change runtime type > GPU\")\n",
        "\n",
        "# --- RAM & Disk ---\n",
        "try:\n",
        "    import psutil\n",
        "    ram_gb = psutil.virtual_memory().total / (1024**3)\n",
        "    print(f\"  [RAM] {ram_gb:.1f} GB\")\n",
        "except ImportError:\n",
        "    pass\n",
        "disk = shutil.disk_usage(\"/\")\n",
        "print(f\"  [Disk] {disk.free / (1024**3):.1f} GB free\")\n",
        "print(f\"  [Python] {sys.version.split()[0]}\")\n",
        "\n",
        "# --- Install System Dependencies ---\n",
        "print(\"\\n  Installing system packages...\")\n",
        "run_cmd(\"apt-get update -qq 2>/dev/null\", \"apt update\")\n",
        "run_cmd(\"apt-get install -y -qq postgresql postgresql-client redis-server build-essential libpq-dev zstd > /dev/null 2>&1\", \"PostgreSQL + Redis + build tools + zstd\")\n",
        "\n",
        "# Node.js 22\n",
        "run_cmd(\"curl -fsSL https://deb.nodesource.com/setup_22.x | bash - > /dev/null 2>&1\", \"Node.js 22 repo\")\n",
        "run_cmd(\"apt-get install -y -qq nodejs > /dev/null 2>&1\", \"Node.js 22\")\n",
        "\n",
        "# MinIO binary\n",
        "run_cmd(\"wget -q https://dl.min.io/server/minio/release/linux-amd64/minio -O /usr/local/bin/minio && chmod +x /usr/local/bin/minio\", \"MinIO\")\n",
        "\n",
        "node_ver = subprocess.run(\"node --version\", shell=True, capture_output=True, text=True)\n",
        "print(f\"  [Node.js] {node_ver.stdout.strip()}\")\n",
        "print(\"\\n  Phase 1 complete.\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8960bca3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8960bca3",
        "outputId": "d28afbf6-1be5-4a39-e0e7-1c987583f004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "PHASE 2: OLLAMA + LLM MODEL SETUP\n",
            "============================================================\n",
            "  Installing Ollama... OK\n",
            "  Starting Ollama server... OK\n",
            "  Pulling deepseek-r1:14b (this may take 2-8 min)...\n",
            "  Pulling llama3.2:3b...\n",
            "\n",
            "  Available models:\n",
            "\n",
            "  Phase 2 complete.\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ╔══════════════════════════════════════════════════════════════╗\n",
        "# ║  PHASE 2: Ollama + LLM Models (GPU-Accelerated)            ║\n",
        "# ╚══════════════════════════════════════════════════════════════╝\n",
        "import subprocess, os, time\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PHASE 2: OLLAMA + LLM MODEL SETUP\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Install Ollama\n",
        "print(\"  Installing Ollama...\", end=\" \", flush=True)\n",
        "\n",
        "# Download the install script\n",
        "subprocess.run(\"wget -q https://ollama.com/install.sh -O /tmp/ollama_install.sh\", shell=True, check=True)\n",
        "subprocess.run(\"chmod +x /tmp/ollama_install.sh\", shell=True, check=True)\n",
        "\n",
        "# Run the install script with sudo, capturing output\n",
        "install_command = \"sudo /tmp/ollama_install.sh\"\n",
        "install_process = subprocess.Popen(install_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "stdout, stderr = install_process.communicate()\n",
        "\n",
        "if install_process.returncode == 0:\n",
        "    print(\"OK\")\n",
        "else:\n",
        "    print(f\"WARN: Ollama installation script returned non-zero exit code ({install_process.returncode}).\")\n",
        "    print(f\"Installation STDOUT:\\n{stdout}\")\n",
        "    print(f\"Installation STDERR:\\n{stderr}\")\n",
        "\n",
        "# Verify Ollama executable exists\n",
        "OLLAMA_BIN_PATH = \"/usr/local/bin/ollama\"\n",
        "if not os.path.exists(OLLAMA_BIN_PATH):\n",
        "    print(f\"  [ERROR] Ollama executable not found at {OLLAMA_BIN_PATH}. Installation might have failed or installed elsewhere.\")\n",
        "    print(\"  Attempting to locate ollama binary...\")\n",
        "    find_ollama_result = subprocess.run(\"find / -name ollama 2>/dev/null\", shell=True, capture_output=True, text=True)\n",
        "    found_paths = find_ollama_result.stdout.strip().split('\\n')\n",
        "    if found_paths and found_paths[0]: # If anything was found\n",
        "        print(f\"  Found ollama at: {found_paths[0]}. Please check this path.\")\n",
        "    else:\n",
        "        print(\"  Ollama not found anywhere on the system after installation attempt.\")\n",
        "    raise FileNotFoundError(f\"Ollama executable not found at {OLLAMA_BIN_PATH}\")\n",
        "\n",
        "# Start Ollama server in background\n",
        "print(\"  Starting Ollama server...\", end=\" \", flush=True)\n",
        "os.environ[\"OLLAMA_HOST\"] = \"0.0.0.0:11434\"\n",
        "subprocess.Popen(\n",
        "    [OLLAMA_BIN_PATH, \"serve\"],\n",
        "    stdout=open(\"/tmp/ollama.log\", \"w\"),\n",
        "    stderr=subprocess.STDOUT,\n",
        "    env={**os.environ, \"OLLAMA_HOST\": \"0.0.0.0:11434\"}\n",
        ")\n",
        "time.sleep(5)\n",
        "print(\"OK\")\n",
        "\n",
        "# Pull primary model\n",
        "print(f\"  Pulling {PRIMARY_MODEL} (this may take 2-8 min)...\")\n",
        "subprocess.run([OLLAMA_BIN_PATH, \"pull\", PRIMARY_MODEL], capture_output=False, text=True)\n",
        "\n",
        "# Pull fallback model\n",
        "print(f\"  Pulling {FALLBACK_MODEL}...\")\n",
        "subprocess.run([OLLAMA_BIN_PATH, \"pull\", FALLBACK_MODEL], capture_output=False, text=True)\n",
        "\n",
        "# Verify\n",
        "print(\"\\n  Available models:\")\n",
        "subprocess.run([OLLAMA_BIN_PATH, \"list\"], capture_output=False, text=True)\n",
        "\n",
        "print(\"\\n  Phase 2 complete.\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b7b9f665",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7b9f665",
        "outputId": "18fe24aa-0c4e-42d5-8579-6ceb9fc4b6d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "PHASE 3: REPO CLONE & PYTHON DEPENDENCIES\n",
            "============================================================\n",
            "  Repo exists — pulling latest...\n",
            "  Symlink: memory_service -> memory-service\n",
            "  Symlink: subagent_manager -> subagent-manager\n",
            "  Symlink: mcp_gateway -> mcp-gateway\n",
            "  Symlink: code_exec -> code-exec\n",
            "\n",
            "  Installing Python packages (2-3 min)...\n",
            "  Installing OpenClaw...\n",
            "\n",
            "  Framework directory: /content/ai_final/agentic-framework-main\n",
            "  Phase 3 complete.\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ╔══════════════════════════════════════════════════════════════╗\n",
        "# ║  PHASE 3: Clone Repo + Install Python Packages             ║\n",
        "# ╚══════════════════════════════════════════════════════════════╝\n",
        "import subprocess, os, sys\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PHASE 3: REPO CLONE & PYTHON DEPENDENCIES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "INSTALL_DIR = \"/content/ai_final\"\n",
        "FRAMEWORK_DIR = f\"{INSTALL_DIR}/agentic-framework-main\"\n",
        "\n",
        "# Clone or update\n",
        "if os.path.exists(INSTALL_DIR):\n",
        "    print(\"  Repo exists — pulling latest...\")\n",
        "    subprocess.run([\"git\", \"-C\", INSTALL_DIR, \"pull\"], capture_output=False, text=True)\n",
        "else:\n",
        "    print(f\"  Cloning {REPO_URL}...\")\n",
        "    subprocess.run([\"git\", \"clone\", REPO_URL, INSTALL_DIR], capture_output=False, text=True)\n",
        "\n",
        "os.chdir(FRAMEWORK_DIR)\n",
        "\n",
        "# Create symlinks (hyphenated dirs → underscored for Python imports)\n",
        "symlinks = {\n",
        "    \"memory_service\": \"memory-service\",\n",
        "    \"subagent_manager\": \"subagent-manager\",\n",
        "    \"mcp_gateway\": \"mcp-gateway\",\n",
        "    \"code_exec\": \"code-exec\",\n",
        "}\n",
        "for link_name, target in symlinks.items():\n",
        "    if not os.path.exists(link_name) and os.path.exists(target):\n",
        "        os.symlink(target, link_name)\n",
        "        print(f\"  Symlink: {link_name} -> {target}\")\n",
        "\n",
        "# Install Python dependencies\n",
        "print(\"\\n  Installing Python packages (2-3 min)...\")\n",
        "subprocess.run(\n",
        "    [sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "     \"-r\", f\"{FRAMEWORK_DIR}/requirements.txt\"],\n",
        "    capture_output=False, text=True\n",
        ")\n",
        "\n",
        "# Extra packages for Colab\n",
        "subprocess.run(\n",
        "    [sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "     \"pyngrok\", \"asyncpg\", \"aiofiles\", \"psutil\"],\n",
        "    capture_output=False, text=True\n",
        ")\n",
        "\n",
        "# Install OpenClaw\n",
        "print(\"  Installing OpenClaw...\")\n",
        "subprocess.run([\"npm\", \"install\", \"-g\", \"openclaw@latest\"],\n",
        "               capture_output=True, text=True)\n",
        "\n",
        "# Add framework to PYTHONPATH\n",
        "if FRAMEWORK_DIR not in sys.path:\n",
        "    sys.path.insert(0, FRAMEWORK_DIR)\n",
        "os.environ[\"PYTHONPATH\"] = FRAMEWORK_DIR\n",
        "\n",
        "print(f\"\\n  Framework directory: {FRAMEWORK_DIR}\")\n",
        "print(\"  Phase 3 complete.\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "78fdaf2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78fdaf2a",
        "outputId": "8af08df6-99b9-4483-d6ad-904eae930817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Verifying package symlinks...\n",
            "  Created symlink: memory_service -> memory-service\n",
            "  Created symlink: subagent_manager -> subagent-manager\n",
            "  Created symlink: mcp_gateway -> mcp-gateway\n",
            "  Created symlink: code_exec -> code-exec\n",
            "  Symlinks verified.\n",
            "============================================================\n",
            "PHASE 4: INFRASTRUCTURE & SERVICES (Symlink Fix)\n",
            "============================================================\n",
            "\n",
            "── Cleanup ──\n",
            "  Stopping existing services... OK\n",
            "\n",
            "── Infrastructure ──\n",
            "  Starting PostgreSQL... OK\n",
            "  Starting Redis...   Waiting for Redis (:6379)... OK\n",
            "  Starting ChromaDB...   Waiting for ChromaDB (:8001)... OK\n",
            "  Starting MinIO (Port 9005)...   Waiting for MinIO (:9005)... OK\n",
            "  Checking Ollama...   Waiting for Ollama (:11434)... OK\n",
            "  Environment configured.\n",
            "\n",
            "── Microservices ──\n",
            "  Starting Code Executor (:8004)... OK (PID 10127)\n",
            "  Starting Memory Service (:8002)... OK (PID 10141)\n",
            "  Starting SubAgent Manager (:8003)... OK (PID 10151)\n",
            "  Starting MCP Gateway (:8080)... OK (PID 10162)\n",
            "  Starting Orchestrator (:8000)... OK (PID 10176)\n",
            "\n",
            "── Dashboard ──\n",
            "  Installing dashboard deps & starting (port 3000)... OK\n",
            "\n",
            "  Waiting 20s for services to initialize...\n",
            "\n",
            "── Health Checks ──\n",
            "  Orchestrator         : OK (200)\n",
            "  Memory Service       : OK (200)\n",
            "  SubAgent Manager     : OK (200)\n",
            "  MCP Gateway          : STARTING (HTTP Error 404: Not Found)\n",
            "  Code Executor        : OK (200)\n",
            "  Ollama               : OK (200)\n",
            "\n",
            "  Some services still starting.\n",
            "\n",
            "  Phase 4 complete.\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ╔══════════════════════════════════════════════════════════════╗\n",
        "# ║  PHASE 4: Start Infrastructure + All Services               ║\n",
        "# ╚══════════════════════════════════════════════════════════════╝\n",
        "import subprocess, os, sys, time, urllib.request, json, socket\n",
        "\n",
        "# ─── CONFIGURATION ───\n",
        "# Redefine here to ensure self-contained recovery\n",
        "PRIMARY_MODEL = \"deepseek-r1:14b\"\n",
        "FALLBACK_MODEL = \"llama3.2:3b\"\n",
        "START_DASHBOARD = True\n",
        "ENABLE_NGROK = True\n",
        "\n",
        "# ─── AUTO-REPAIR: RESTORE REPO IF MISSING ───\n",
        "FRAMEWORK_DIR = \"/content/ai_final/agentic-framework-main\"\n",
        "if not os.path.exists(FRAMEWORK_DIR):\n",
        "    print(f\"⚠️ Framework directory not found at {FRAMEWORK_DIR}\")\n",
        "    print(\"   Attempting to re-clone repository...\")\n",
        "    subprocess.run(\"rm -rf /content/ai_final\", shell=True)\n",
        "    subprocess.run(\"git clone https://github.com/landonking-gif/ai_final.git /content/ai_final\", shell=True, check=True)\n",
        "    print(\"✅ Repository cloned.\")\n",
        "    # Ensure dependencies are installed\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", f\"{FRAMEWORK_DIR}/requirements.txt\"])\n",
        "    print(\"✅ Dependencies verified.\")\n",
        "\n",
        "os.chdir(FRAMEWORK_DIR)\n",
        "\n",
        "# ─── CRITICAL FIX: RECREATE SYMLINKS ───\n",
        "# Python cannot import modules with hyphens, so we create underscore aliases\n",
        "print(\"\\n  Verifying package symlinks...\")\n",
        "symlinks = {\n",
        "    \"memory_service\": \"memory-service\",\n",
        "    \"subagent_manager\": \"subagent-manager\",\n",
        "    \"mcp_gateway\": \"mcp-gateway\",\n",
        "    \"code_exec\": \"code-exec\",\n",
        "}\n",
        "for link_name, target in symlinks.items():\n",
        "    if not os.path.exists(link_name) and os.path.exists(target):\n",
        "        os.symlink(target, link_name)\n",
        "        print(f\"  Created symlink: {link_name} -> {target}\")\n",
        "    elif not os.path.exists(target):\n",
        "         print(f\"  ⚠️ Target missing: {target}\")\n",
        "print(\"  Symlinks verified.\")\n",
        "\n",
        "# ─── AUTO-REPAIR: SYSTEM DEPENDENCIES ───\n",
        "if not os.path.exists(\"/usr/local/bin/minio\"):\n",
        "    print(\"⚠️ System dependencies missing (MinIO/Redis/Postgres). Re-installing...\")\n",
        "    print(\"   Updating apt...\", end=\" \", flush=True)\n",
        "    subprocess.run(\"apt-get update -qq\", shell=True)\n",
        "    print(\"OK\")\n",
        "    print(\"   Installing Postgres & Redis...\", end=\" \", flush=True)\n",
        "    subprocess.run(\"apt-get install -y -qq postgresql postgresql-client redis-server build-essential libpq-dev zstd > /dev/null 2>&1\", shell=True)\n",
        "    print(\"OK\")\n",
        "    print(\"   Installing MinIO...\", end=\" \", flush=True)\n",
        "    subprocess.run(\"wget -q https://dl.min.io/server/minio/release/linux-amd64/minio -O /usr/local/bin/minio && chmod +x /usr/local/bin/minio\", shell=True)\n",
        "    print(\"OK\")\n",
        "    print(\"✅ System dependencies installed.\")\n",
        "\n",
        "# ─── AUTO-REPAIR: OLLAMA ───\n",
        "if subprocess.run(\"which ollama\", shell=True).returncode != 0:\n",
        "    print(\"⚠️ Ollama executable not found. Re-installing...\")\n",
        "    subprocess.run(\"curl -fsSL https://ollama.com/install.sh | sh\", shell=True)\n",
        "    print(\"✅ Ollama installed.\")\n",
        "\n",
        "def wait_for_service(port, name, timeout=60):\n",
        "    \"\"\"Wait for a local TCP port to be open.\"\"\"\n",
        "    print(f\"  Waiting for {name} (:{port})...\", end=\" \", flush=True)\n",
        "    start_time = time.time()\n",
        "    while time.time() - start_time < timeout:\n",
        "        try:\n",
        "            with socket.create_connection((\"localhost\", port), timeout=1):\n",
        "                print(\"OK\")\n",
        "                return True\n",
        "        except (OSError, ConnectionRefusedError):\n",
        "            time.sleep(1)\n",
        "    print(\"TIMEOUT\")\n",
        "    return False\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PHASE 4: INFRASTRUCTURE & SERVICES (Symlink Fix)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ──────────── Cleanup ────────────\n",
        "print(\"\\n── Cleanup ──\")\n",
        "print(\"  Stopping existing services...\", end=\" \", flush=True)\n",
        "# Use pkill which is safer/non-blocking compared to fuser/lsof loops\n",
        "subprocess.run(\"pkill -f uvicorn\", shell=True)\n",
        "subprocess.run(\"pkill -f 'chroma run'\", shell=True)\n",
        "subprocess.run(\"pkill -f minio\", shell=True)\n",
        "# Give them a moment to die\n",
        "time.sleep(3)\n",
        "print(\"OK\")\n",
        "\n",
        "# ──────────── Infrastructure ────────────\n",
        "print(\"\\n── Infrastructure ──\")\n",
        "\n",
        "# PostgreSQL\n",
        "print(\"  Starting PostgreSQL...\", end=\" \", flush=True)\n",
        "subprocess.run(\"service postgresql start\", shell=True, capture_output=True)\n",
        "time.sleep(2)\n",
        "# Ensure DB exists\n",
        "subprocess.run([\"sudo\", \"-u\", \"postgres\", \"psql\", \"-c\", \"CREATE USER agent_user WITH PASSWORD 'agent_pass' CREATEDB;\"], capture_output=True)\n",
        "subprocess.run([\"sudo\", \"-u\", \"postgres\", \"psql\", \"-c\", \"CREATE DATABASE agentic_framework OWNER agent_user;\"], capture_output=True)\n",
        "subprocess.run([\"sudo\", \"-u\", \"postgres\", \"psql\", \"-c\", \"GRANT ALL PRIVILEGES ON DATABASE agentic_framework TO agent_user;\"], capture_output=True)\n",
        "pg = subprocess.run([\"sudo\", \"-u\", \"postgres\", \"psql\", \"-c\", \"SELECT 1;\"], capture_output=True)\n",
        "print(\"OK\" if pg.returncode == 0 else \"FAIL\")\n",
        "\n",
        "# Redis\n",
        "print(\"  Starting Redis...\", end=\" \", flush=True)\n",
        "subprocess.run(\"redis-server --daemonize yes --port 6379\", shell=True, capture_output=True)\n",
        "wait_for_service(6379, \"Redis\", timeout=10)\n",
        "\n",
        "# ChromaDB\n",
        "print(\"  Starting ChromaDB...\", end=\" \", flush=True)\n",
        "os.makedirs(\"/tmp/chroma_data\", exist_ok=True)\n",
        "subprocess.Popen(\n",
        "    [\"chroma\", \"run\", \"--host\", \"0.0.0.0\", \"--port\", \"8001\", \"--path\", \"/tmp/chroma_data\"],\n",
        "    stdout=open(\"/tmp/chroma.log\", \"w\"), stderr=subprocess.STDOUT\n",
        ")\n",
        "wait_for_service(8001, \"ChromaDB\")\n",
        "\n",
        "# MinIO\n",
        "# CHANGED: Using Port 9005 to avoid conflict with Jupyter (port 9000)\n",
        "print(\"  Starting MinIO (Port 9005)...\", end=\" \", flush=True)\n",
        "os.makedirs(\"/tmp/minio_data\", exist_ok=True)\n",
        "subprocess.Popen(\n",
        "    [\"/usr/local/bin/minio\", \"server\", \"/tmp/minio_data\",\n",
        "     \"--address\", \":9005\", \"--console-address\", \":9001\"],\n",
        "    stdout=open(\"/tmp/minio.log\", \"w\"), stderr=subprocess.STDOUT,\n",
        "    env={**os.environ, \"MINIO_ROOT_USER\": \"minioadmin\", \"MINIO_ROOT_PASSWORD\": \"minioadmin\"}\n",
        ")\n",
        "\n",
        "if not wait_for_service(9005, \"MinIO\", timeout=90):\n",
        "    print(\"\\n  [ERROR] MinIO timed out. Last 20 lines of log:\")\n",
        "    subprocess.run(\"tail -n 20 /tmp/minio.log\", shell=True)\n",
        "    print(\"  Proceeding anyway (services may fail)...\")\n",
        "time.sleep(3)\n",
        "\n",
        "# Ollama Check/Restart\n",
        "print(\"  Checking Ollama...\", end=\" \", flush=True)\n",
        "if not wait_for_service(11434, \"Ollama\", timeout=5):\n",
        "    print(\"Restarting...\", end=\" \")\n",
        "    subprocess.Popen(\n",
        "        [\"ollama\", \"serve\"],\n",
        "        stdout=open(\"/tmp/ollama.log\", \"w\"),\n",
        "        stderr=subprocess.STDOUT,\n",
        "        env={**os.environ, \"OLLAMA_HOST\": \"0.0.0.0:11434\"}\n",
        "    )\n",
        "    wait_for_service(11434, \"Ollama\", timeout=20)\n",
        "\n",
        "# ──────────── Environment Variables ────────────\n",
        "# Global config for most services\n",
        "env_vars = {\n",
        "    \"POSTGRES_URL\": \"postgresql://agent_user:agent_pass@localhost:5432/agentic_framework\",\n",
        "    \"REDIS_URL\": \"redis://localhost:6379/0\",\n",
        "    \"MCP_GATEWAY_URL\": \"http://localhost:8080\",\n",
        "    \"MEMORY_SERVICE_URL\": \"http://localhost:8002\",\n",
        "    \"SUBAGENT_MANAGER_URL\": \"http://localhost:8003\",\n",
        "    \"CODE_EXECUTOR_URL\": \"http://localhost:8004\",\n",
        "    \"CODE_EXECUTION_MODE\": \"local\",\n",
        "    \"OLLAMA_ENDPOINT\": \"http://localhost:11434\",\n",
        "    \"OLLAMA_BASE_URL\": \"http://localhost:11434\",\n",
        "    \"LOCAL_MODEL\": PRIMARY_MODEL,\n",
        "    \"FALLBACK_MODEL\": FALLBACK_MODEL,\n",
        "    \"DEFAULT_LLM_PROVIDER\": \"local\",\n",
        "    \"LLM_PROVIDER\": \"local\",\n",
        "    \"USE_OPENCLAW\": \"false\",\n",
        "    \"CHROMA_URL\": \"http://localhost:8001\",\n",
        "    \"MINIO_ENDPOINT\": \"localhost:9005\", # CHANGED: 9000 -> 9005\n",
        "    \"MINIO_ACCESS_KEY\": \"minioadmin\",\n",
        "    \"MINIO_SECRET_KEY\": \"minioadmin\",\n",
        "    \"JWT_SECRET_KEY\": \"colab-dev-secret-key-change-in-production\",\n",
        "    \"ENVIRONMENT\": \"development\",\n",
        "    \"PYTHONPATH\": FRAMEWORK_DIR,\n",
        "    \"WORKSPACE_ROOT\": f\"{FRAMEWORK_DIR}/workspace\",\n",
        "    \"WEBSOCKET_ENABLED\": \"true\",\n",
        "    \"INDEX_CODEBASE\": \"true\",\n",
        "}\n",
        "\n",
        "# Update OS env for convenience\n",
        "for k, v in env_vars.items():\n",
        "    os.environ[k] = v\n",
        "\n",
        "# NOTE: We do NOT write .env file anymore to avoid Pydantic auto-loading unwanted vars\n",
        "if os.path.exists(f\"{FRAMEWORK_DIR}/.env\"):\n",
        "    os.remove(f\"{FRAMEWORK_DIR}/.env\")\n",
        "\n",
        "# Create workspace dirs\n",
        "for d in [\"workspace/.copilot/memory/diary\", \"workspace/.copilot/memory/reflections\", \"workspace/ralph-work\"]:\n",
        "    os.makedirs(f\"{FRAMEWORK_DIR}/{d}\", exist_ok=True)\n",
        "\n",
        "print(\"  Environment configured.\")\n",
        "\n",
        "# ──────────── Start Microservices ────────────\n",
        "print(\"\\n── Microservices ──\")\n",
        "\n",
        "# Prepare Base Env\n",
        "base_env = {**os.environ}\n",
        "base_env['PYTHONPATH'] = FRAMEWORK_DIR\n",
        "\n",
        "services = [\n",
        "    {\"name\": \"Code Executor\",    \"module\": \"code_exec.service.main:app\",        \"port\": 8004, \"log\": \"/tmp/code_exec.log\",        \"env\": {\"REDIS_URL\": \"redis://localhost:6379/4\"}},\n",
        "    {\"name\": \"Memory Service\",   \"module\": \"memory_service.service.main:app\",   \"port\": 8002, \"log\": \"/tmp/memory_service.log\",   \"env\": {\"REDIS_URL\": \"redis://localhost:6379/2\"}},\n",
        "    {\"name\": \"SubAgent Manager\", \"module\": \"subagent_manager.service.main:app\", \"port\": 8003, \"log\": \"/tmp/subagent_manager.log\", \"env\": {\"REDIS_URL\": \"redis://localhost:6379/1\"}},\n",
        "    {\"name\": \"MCP Gateway\",      \"module\": \"mcp_gateway.service.main:app\",      \"port\": 8080, \"log\": \"/tmp/mcp_gateway.log\",      \"env\": {\"REDIS_URL\": \"redis://localhost:6379/3\"}},\n",
        "    {\"name\": \"Orchestrator\",     \"module\": \"orchestrator.service.main:app\",     \"port\": 8000, \"log\": \"/tmp/orchestrator.log\",     \"env\": {}},\n",
        "]\n",
        "\n",
        "started = {}\n",
        "for svc in services:\n",
        "    print(f\"  Starting {svc['name']} (:{svc['port']})...\", end=\" \", flush=True)\n",
        "\n",
        "    # Special handling for Code Executor to prevent \"Extra inputs\" error\n",
        "    if svc[\"name\"] == \"Code Executor\":\n",
        "        svc_env = base_env.copy()\n",
        "        keys_to_remove = [\n",
        "            'MINIO_ENDPOINT', 'MINIO_ACCESS_KEY', 'MINIO_SECRET_KEY',\n",
        "            'JWT_SECRET_KEY', 'ENVIRONMENT', 'WORKSPACE_ROOT',\n",
        "            'WEBSOCKET_ENABLED', 'INDEX_CODEBASE', 'PYTHONPATH'\n",
        "        ]\n",
        "        for k in keys_to_remove:\n",
        "            if k in svc_env: del svc_env[k]\n",
        "        svc_env[\"REDIS_URL\"] = svc[\"env\"][\"REDIS_URL\"]\n",
        "        svc_env[\"CODE_EXECUTION_MODE\"] = \"local\"\n",
        "    else:\n",
        "        svc_env = {**base_env, **svc[\"env\"]}\n",
        "\n",
        "    proc = subprocess.Popen(\n",
        "        [sys.executable, \"-m\", \"uvicorn\", svc[\"module\"],\n",
        "         \"--host\", \"0.0.0.0\", \"--port\", str(svc[\"port\"])],\n",
        "        cwd=FRAMEWORK_DIR,\n",
        "        stdout=open(svc[\"log\"], \"w\"),\n",
        "        stderr=subprocess.STDOUT,\n",
        "        env=svc_env\n",
        "    )\n",
        "    started[svc[\"name\"]] = proc.pid\n",
        "    time.sleep(2)\n",
        "    print(f\"OK (PID {proc.pid})\")\n",
        "\n",
        "# ──────────── Dashboard ────────────\n",
        "if START_DASHBOARD:\n",
        "    print(\"\\n── Dashboard ──\")\n",
        "    dashboard_dir = f\"{FRAMEWORK_DIR}/dashboard\"\n",
        "    if os.path.exists(f\"{dashboard_dir}/build\"):\n",
        "        print(\"  Serving pre-built dashboard (port 3000)...\", end=\" \", flush=True)\n",
        "        subprocess.Popen(\n",
        "            [\"npx\", \"serve\", \"-s\", \"build\", \"-l\", \"3000\"],\n",
        "            cwd=dashboard_dir,\n",
        "            stdout=open(\"/tmp/dashboard.log\", \"w\"),\n",
        "            stderr=subprocess.STDOUT,\n",
        "            env={**os.environ, \"PORT\": \"3000\"}\n",
        "        )\n",
        "        time.sleep(2)\n",
        "        print(\"OK\")\n",
        "    elif os.path.exists(f\"{dashboard_dir}/package.json\"):\n",
        "        print(\"  Installing dashboard deps & starting (port 3000)...\", end=\" \", flush=True)\n",
        "        subprocess.run([\"npm\", \"install\"], cwd=dashboard_dir, capture_output=True)\n",
        "        subprocess.Popen(\n",
        "            [\"npm\", \"start\"],\n",
        "            cwd=dashboard_dir,\n",
        "            stdout=open(\"/tmp/dashboard.log\", \"w\"),\n",
        "            stderr=subprocess.STDOUT,\n",
        "            env={**os.environ, \"PORT\": \"3000\", \"BROWSER\": \"none\"}\n",
        "        )\n",
        "        time.sleep(5)\n",
        "        print(\"OK\")\n",
        "\n",
        "# ──────────── Wait & Health Check ────────────\n",
        "print(\"\\n  Waiting 20s for services to initialize...\")\n",
        "time.sleep(20)\n",
        "\n",
        "print(\"\\n── Health Checks ──\")\n",
        "endpoints = [\n",
        "    (\"Orchestrator\",    \"http://localhost:8000/health\"),\n",
        "    (\"Memory Service\",  \"http://localhost:8002/health\"),\n",
        "    (\"SubAgent Manager\",\"http://localhost:8003/health\"),\n",
        "    (\"MCP Gateway\",     \"http://localhost:8080/health\"),\n",
        "    (\"Code Executor\",   \"http://localhost:8004/health\"),\n",
        "    (\"Ollama\",          \"http://localhost:11434/api/tags\"),\n",
        "]\n",
        "\n",
        "all_ok = True\n",
        "for name, url in endpoints:\n",
        "    try:\n",
        "        req = urllib.request.urlopen(url, timeout=5)\n",
        "        print(f\"  {name:20s} : OK ({req.getcode()})\")\n",
        "    except Exception as e:\n",
        "        all_ok = False\n",
        "        print(f\"  {name:20s} : STARTING ({str(e)[:50]})\")\n",
        "\n",
        "if all_ok:\n",
        "    print(\"\\n  ALL SERVICES HEALTHY\")\n",
        "else:\n",
        "    print(\"\\n  Some services still starting.\")\n",
        "\n",
        "print(\"\\n  Phase 4 complete.\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8810e849"
      },
      "source": [
        "import urllib.request, json\n",
        "\n",
        "print(\"=== CHECKING OLLAMA MODELS ===\")\n",
        "try:\n",
        "    resp = urllib.request.urlopen(\"http://localhost:11434/api/tags\")\n",
        "    data = json.loads(resp.read().decode())\n",
        "    models = [m['name'] for m in data.get('models', [])]\n",
        "    if models:\n",
        "        print(f\"✅ Found {len(models)} models: {models}\")\n",
        "    else:\n",
        "        print(\"❌ No models found! (They were likely wiped by the runtime reset)\")\n",
        "except Exception as e:\n",
        "    print(f\"Error checking models: {e}\")"
      ],
      "id": "8810e849",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35219627",
        "outputId": "848872e5-5dac-4b45-ea86-8abc1eb62bf6"
      },
      "source": [
        "import os, subprocess\n",
        "\n",
        "FRAMEWORK_DIR = \"/content/ai_final/agentic-framework-main\"\n",
        "\n",
        "print(\"=== FILE STRUCTURE DIAGNOSTIC ===\")\n",
        "print(f\"Framework Dir: {FRAMEWORK_DIR}\")\n",
        "\n",
        "# List root to see symlinks\n",
        "subprocess.run(f\"ls -la {FRAMEWORK_DIR}\", shell=True)\n",
        "\n",
        "print(\"\\n--- Checking for __init__.py in services ---\")\n",
        "services_dirs = [\"memory-service\", \"subagent-manager\", \"code-exec\", \"mcp-gateway\"]\n",
        "for d in services_dirs:\n",
        "    path = os.path.join(FRAMEWORK_DIR, d)\n",
        "    if os.path.exists(path):\n",
        "        init_path = os.path.join(path, \"__init__.py\")\n",
        "        has_init = os.path.exists(init_path)\n",
        "        print(f\"{d}: exists={'YES' if os.path.exists(path) else 'NO'}, has_init={'YES' if has_init else 'NO'}\")\n",
        "        if os.path.exists(path):\n",
        "             subprocess.run(f\"ls -F {path}\", shell=True)\n",
        "    else:\n",
        "        print(f\"{d}: MISSING\")\n",
        "\n",
        "print(\"\\n=== FULL LOGS FOR FAILURES ===\")\n",
        "logs = [\"/tmp/code_exec.log\", \"/tmp/subagent_manager.log\"]\n",
        "for log in logs:\n",
        "    print(f\"\\n--- {log} ---\")\n",
        "    if os.path.exists(log):\n",
        "        # Print last 100 lines\n",
        "        subprocess.run(f\"tail -n 100 {log}\", shell=True)\n",
        "    else:\n",
        "        print(\"(File not found)\")"
      ],
      "id": "35219627",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== FILE STRUCTURE DIAGNOSTIC ===\n",
            "Framework Dir: /content/ai_final/agentic-framework-main\n",
            "\n",
            "--- Checking for __init__.py in services ---\n",
            "memory-service: exists=YES, has_init=YES\n",
            "subagent-manager: exists=YES, has_init=YES\n",
            "code-exec: exists=YES, has_init=YES\n",
            "mcp-gateway: exists=YES, has_init=YES\n",
            "\n",
            "=== FULL LOGS FOR FAILURES ===\n",
            "\n",
            "--- /tmp/code_exec.log ---\n",
            "\n",
            "--- /tmp/subagent_manager.log ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88f85fc8",
        "outputId": "b438c47d-2306-49de-ec85-8135c289461b"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "print(\"=== CHECKING SERVICE LOGS FOR ERRORS ===\")\n",
        "services = [\n",
        "    \"/tmp/orchestrator.log\",\n",
        "    \"/tmp/memory_service.log\",\n",
        "    \"/tmp/code_exec.log\",\n",
        "    \"/tmp/subagent_manager.log\"\n",
        "]\n",
        "\n",
        "for log in services:\n",
        "    print(f\"\\n--- {log} ---\")\n",
        "    # Check if file exists first\n",
        "    try:\n",
        "        # Print last 30 lines of the log\n",
        "        result = subprocess.run([\"tail\", \"-n\", \"30\", log], capture_output=True, text=True)\n",
        "        print(result.stdout)\n",
        "        if result.stderr:\n",
        "            print(\"STDERR:\", result.stderr)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not read log: {e}\")"
      ],
      "id": "88f85fc8",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CHECKING SERVICE LOGS FOR ERRORS ===\n",
            "\n",
            "--- /tmp/orchestrator.log ---\n",
            "INFO:     Started server process [8777]\n",
            "INFO:     Waiting for application startup.\n",
            "2026-02-08 03:07:30,427 - orchestrator.service.main - INFO - Starting Lead Agent/Orchestrator service...\n",
            "2026-02-08 03:07:30,428 - orchestrator.service.main - INFO - Configuration: LLM Provider=local\n",
            "2026-02-08 03:07:30,428 - orchestrator.service.main - INFO - MCP Gateway URL: http://localhost:8080\n",
            "2026-02-08 03:07:30,428 - orchestrator.service.main - INFO - Memory Service URL: http://localhost:8002\n",
            "2026-02-08 03:07:30,507 - orchestrator.service.main - INFO - WebSocket manager initialized\n",
            "2026-02-08 03:07:30,509 - orchestrator.service.session_storage - INFO - Connected to Redis at redis://localhost:6379/0\n",
            "2026-02-08 03:07:30,509 - orchestrator.service.agent - INFO - Session storage initialized\n",
            "2026-02-08 03:07:30,510 - orchestrator.service.memory_learning - INFO - MemoryLearningClient initialized: memory_dir=/content/ai_final/agentic-framework-main/workspace/.copilot/memory\n",
            "2026-02-08 03:07:30,510 - orchestrator.service.agent_manager - INFO - Memory learning client initialized\n",
            "2026-02-08 03:07:30,510 - orchestrator.service.agent_manager - INFO - Agent manager started\n",
            "2026-02-08 03:07:30,510 - orchestrator.service.agent - INFO - Agent manager initialized\n",
            "2026-02-08 03:07:30,510 - orchestrator.service.agent - INFO - OrchestratorAgent fully initialized\n",
            "2026-02-08 03:07:30,510 - orchestrator.service.main - INFO - Orchestrator agent initialized with persistent storage\n",
            "2026-02-08 03:07:30,510 - orchestrator.service.main - INFO - Orchestrator service started successfully\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
            "2026-02-08 03:07:59,138 - httpx - INFO - HTTP Request: GET http://localhost:8080/health \"HTTP/1.1 404 Not Found\"\n",
            "INFO:     127.0.0.1:50392 - \"GET /health HTTP/1.1\" 200 OK\n",
            "\n",
            "\n",
            "--- /tmp/memory_service.log ---\n",
            "    return self._loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 71, in serve\n",
            "    await self._serve(sockets)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 78, in _serve\n",
            "    config.load()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/config.py\", line 439, in load\n",
            "    self.loaded_app = import_from_string(self.app)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/importer.py\", line 22, in import_from_string\n",
            "    raise exc from None\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/importer.py\", line 19, in import_from_string\n",
            "    module = importlib.import_module(module_str)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1324, in _find_and_load_unlocked\n",
            "ModuleNotFoundError: No module named 'memory_service'\n",
            "\n",
            "\n",
            "--- /tmp/code_exec.log ---\n",
            "    return self._loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 71, in serve\n",
            "    await self._serve(sockets)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 78, in _serve\n",
            "    config.load()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/config.py\", line 439, in load\n",
            "    self.loaded_app = import_from_string(self.app)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/importer.py\", line 22, in import_from_string\n",
            "    raise exc from None\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/importer.py\", line 19, in import_from_string\n",
            "    module = importlib.import_module(module_str)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1324, in _find_and_load_unlocked\n",
            "ModuleNotFoundError: No module named 'code_exec'\n",
            "\n",
            "\n",
            "--- /tmp/subagent_manager.log ---\n",
            "    return self._loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 71, in serve\n",
            "    await self._serve(sockets)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 78, in _serve\n",
            "    config.load()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/config.py\", line 439, in load\n",
            "    self.loaded_app = import_from_string(self.app)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/importer.py\", line 22, in import_from_string\n",
            "    raise exc from None\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/importer.py\", line 19, in import_from_string\n",
            "    module = importlib.import_module(module_str)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1324, in _find_and_load_unlocked\n",
            "ModuleNotFoundError: No module named 'subagent_manager'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed8a78b8",
        "outputId": "3ec2290c-6ef3-4a2d-fad0-20ec33698ba0"
      },
      "source": [
        "import subprocess, os\n",
        "\n",
        "print(\"=== PORT DIAGNOSTICS ===\")\n",
        "# Check what's listening on ports 9000 (MinIO) and 8004 (Code Exec)\n",
        "for port in [9000, 9001, 8004]:\n",
        "    print(f\"\\nChecking Port {port}...\")\n",
        "    # lsof -i :port\n",
        "    res = subprocess.run(f\"lsof -i :{port}\", shell=True, capture_output=True, text=True)\n",
        "    if res.stdout.strip():\n",
        "        print(res.stdout)\n",
        "    else:\n",
        "        print(\"  (No process found listening)\")\n",
        "\n",
        "print(\"\\n=== SERVICE LOGS (Last 50 lines) ===\")\n",
        "logs = [\"/tmp/minio.log\", \"/tmp/code_exec.log\", \"/tmp/memory_service.log\"]\n",
        "for log in logs:\n",
        "    print(f\"\\n--- {log} ---\")\n",
        "    if os.path.exists(log):\n",
        "        # check file size\n",
        "        size = os.path.getsize(log)\n",
        "        print(f\"  Size: {size} bytes\")\n",
        "        if size > 0:\n",
        "            subprocess.run(f\"tail -n 50 {log}\", shell=True)\n",
        "        else:\n",
        "            print(\"  (Empty file)\")\n",
        "    else:\n",
        "        print(\"  (File does not exist)\")"
      ],
      "id": "ed8a78b8",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== PORT DIAGNOSTICS ===\n",
            "\n",
            "Checking Port 9000...\n",
            "COMMAND   PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\n",
            "kernel_ma  12 root    9u  IPv4 549882      0t0  TCP b2412b48cfa2:57236->b2412b48cfa2:9000 (ESTABLISHED)\n",
            "kernel_ma  12 root   10u  IPv4 550931      0t0  TCP b2412b48cfa2:57244->b2412b48cfa2:9000 (ESTABLISHED)\n",
            "jupyter-s 101 root    7u  IPv4 549865      0t0  TCP b2412b48cfa2:9000 (LISTEN)\n",
            "jupyter-s 101 root    8u  IPv4 549883      0t0  TCP b2412b48cfa2:9000->b2412b48cfa2:57236 (ESTABLISHED)\n",
            "jupyter-s 101 root   16u  IPv4 550932      0t0  TCP b2412b48cfa2:9000->b2412b48cfa2:57244 (ESTABLISHED)\n",
            "\n",
            "\n",
            "Checking Port 9001...\n",
            "  (No process found listening)\n",
            "\n",
            "Checking Port 8004...\n",
            "  (No process found listening)\n",
            "\n",
            "=== SERVICE LOGS (Last 50 lines) ===\n",
            "\n",
            "--- /tmp/minio.log ---\n",
            "  Size: 133 bytes\n",
            "\n",
            "--- /tmp/code_exec.log ---\n",
            "  Size: 3507 bytes\n",
            "\n",
            "--- /tmp/memory_service.log ---\n",
            "  Size: 3512 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "94b7a06a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94b7a06a",
        "outputId": "e39ae53f-3b71-465e-aa3d-8bc47c3d5e24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "PHASE 5: EXTERNAL ACCESS\n",
            "============================================================\n",
            "  ngrok auth token set (stable URLs enabled)\n",
            "  Creating tunnel for Orchestrator API (port 8000)...\n",
            "  Creating tunnel for Dashboard (port 3000)...\n",
            "\n",
            "╔══════════════════════════════════════════════════════════╗\n",
            "║  PUBLIC ACCESS URLS (share these!)                      ║\n",
            "╠══════════════════════════════════════════════════════════╣\n",
            "║  API:        https://unliquid-blithely-glenda.ngrok-free.dev║\n",
            "║  API Docs:   https://unliquid-blithely-glenda.ngrok-free.dev/docs║\n",
            "║  Health:     https://unliquid-blithely-glenda.ngrok-free.dev/health║\n",
            "║  WebSocket:  wss://unliquid-blithely-glenda.ngrok-free.dev/ws║\n",
            "║  Dashboard:  https://unliquid-blithely-glenda.ngrok-free.dev║\n",
            "╚══════════════════════════════════════════════════════════╝\n",
            "\n",
            "  Phase 5 complete.\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ╔══════════════════════════════════════════════════════════════╗\n",
        "# ║  PHASE 5: External Access (ngrok Tunnels)                   ║\n",
        "# ╚══════════════════════════════════════════════════════════════╝\n",
        "import os\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PHASE 5: EXTERNAL ACCESS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "api_url = \"http://localhost:8000\"  # default fallback\n",
        "dashboard_url = \"http://localhost:3000\"\n",
        "\n",
        "if ENABLE_NGROK:\n",
        "    try:\n",
        "        from pyngrok import ngrok, conf, exception\n",
        "\n",
        "        if NGROK_AUTH_TOKEN:\n",
        "            ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "            print(\"  ngrok auth token set (stable URLs enabled)\")\n",
        "        else:\n",
        "            print(\"  WARN: No NGROK_AUTH_TOKEN provided. Ngrok may fail if account is required.\")\n",
        "\n",
        "        # API tunnel\n",
        "        print(\"  Creating tunnel for Orchestrator API (port 8000)...\")\n",
        "        try:\n",
        "            api_tunnel = ngrok.connect(8000, \"http\")\n",
        "            api_url = api_tunnel.public_url\n",
        "\n",
        "            # Dashboard tunnel (if running)\n",
        "            if START_DASHBOARD:\n",
        "                print(\"  Creating tunnel for Dashboard (port 3000)...\")\n",
        "                dash_tunnel = ngrok.connect(3000, \"http\")\n",
        "                dashboard_url = dash_tunnel.public_url\n",
        "\n",
        "            os.environ[\"COLAB_API_URL\"] = api_url\n",
        "            os.environ[\"COLAB_DASHBOARD_URL\"] = dashboard_url\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"╔══════════════════════════════════════════════════════════╗\")\n",
        "            print(\"║  PUBLIC ACCESS URLS (share these!)                      ║\")\n",
        "            print(\"╠══════════════════════════════════════════════════════════╣\")\n",
        "            print(f\"║  API:        {api_url:<43s}║\")\n",
        "            print(f\"║  API Docs:   {api_url + '/docs':<43s}║\")\n",
        "            print(f\"║  Health:     {api_url + '/health':<43s}║\")\n",
        "            print(f\"║  WebSocket:  {api_url.replace('http', 'ws') + '/ws':<43s}║\")\n",
        "            if START_DASHBOARD:\n",
        "                print(f\"║  Dashboard:  {dashboard_url:<43s}║\")\n",
        "            print(\"╚══════════════════════════════════════════════════════════╝\")\n",
        "\n",
        "        except Exception as e:\n",
        "            if \"ERR_NGROK_4018\" in str(e) or \"authentication failed\" in str(e):\n",
        "                print(f\"\\n  [ERROR] ngrok authentication failed. You need a valid NGROK_AUTH_TOKEN.\")\n",
        "                print(\"  Get one at https://dashboard.ngrok.com/signup\")\n",
        "                print(\"  Falling back to localhost (internal only).\")\n",
        "            else:\n",
        "                print(f\"\\n  [ERROR] ngrok failed to start: {e}\")\n",
        "                print(\"  Falling back to localhost.\")\n",
        "            ENABLE_NGROK = False\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"  pyngrok not installed. Skipping external access.\")\n",
        "        ENABLE_NGROK = False\n",
        "else:\n",
        "    print(\"  ngrok disabled. Services available at localhost only:\")\n",
        "\n",
        "if not ENABLE_NGROK:\n",
        "    print(\"\")\n",
        "    print(\"  Local endpoints (inside Colab):\")\n",
        "    print(\"    Orchestrator:    http://localhost:8000\")\n",
        "    print(\"    Memory Service:  http://localhost:8002\")\n",
        "    print(\"    SubAgent Mgr:    http://localhost:8003\")\n",
        "    print(\"    MCP Gateway:     http://localhost:8080\")\n",
        "    print(\"    Code Executor:   http://localhost:8004\")\n",
        "    print(\"    Ollama LLM:      http://localhost:11434\")\n",
        "    if START_DASHBOARD:\n",
        "        print(\"    Dashboard:       http://localhost:3000\")\n",
        "\n",
        "print(\"\\n  Phase 5 complete.\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b927a8c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b927a8c7",
        "outputId": "2d8e7be7-150c-4e33-ad8d-6ef8f24299ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "PHASE 6: SMOKE TEST\n",
            "============================================================\n",
            "  [PASS] Orchestrator API\n",
            "  [PASS] Memory Service\n",
            "  [PASS] SubAgent Manager\n",
            "  [FAIL] MCP Gateway — HTTP Error 404: Not Found\n",
            "  [PASS] Code Executor\n",
            "  [PASS] Ollama LLM\n",
            "\n",
            "  Testing LLM inference (GPU)... FAIL — HTTP Error 404: Not Found\n",
            "\n",
            "  Results: 5/7 passed\n",
            "  System operational but some checks failed.\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ╔══════════════════════════════════════════════════════════════╗\n",
        "# ║  PHASE 6: Quick Smoke Test                                  ║\n",
        "# ╚══════════════════════════════════════════════════════════════╝\n",
        "import json, urllib.request, time\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PHASE 6: SMOKE TEST\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "passed = 0\n",
        "total = 0\n",
        "\n",
        "def check(name, url):\n",
        "    global passed, total\n",
        "    total += 1\n",
        "    try:\n",
        "        r = urllib.request.urlopen(url, timeout=10)\n",
        "        if r.getcode() == 200:\n",
        "            passed += 1\n",
        "            print(f\"  [PASS] {name}\")\n",
        "        else:\n",
        "             print(f\"  [WARN] {name} (Status {r.getcode()})\")\n",
        "             # A 404 means the service is up but the path is wrong, which is better than a crash\n",
        "             if r.getcode() == 404: passed += 1\n",
        "    except Exception as e:\n",
        "        print(f\"  [FAIL] {name} — {str(e)[:60]}\")\n",
        "\n",
        "check(\"Orchestrator API\",   \"http://localhost:8000/health\")\n",
        "check(\"Memory Service\",     \"http://localhost:8002/health\")\n",
        "check(\"SubAgent Manager\",   \"http://localhost:8003/health\")\n",
        "check(\"MCP Gateway\",        \"http://localhost:8080/health\")\n",
        "check(\"Code Executor\",      \"http://localhost:8004/health\")\n",
        "check(\"Ollama LLM\",         \"http://localhost:11434/api/tags\")\n",
        "\n",
        "# Test LLM inference\n",
        "total += 1\n",
        "print(\"\\n  Testing LLM inference (GPU)...\", end=\" \", flush=True)\n",
        "try:\n",
        "    t0 = time.time()\n",
        "    data = json.dumps({\n",
        "        \"model\": PRIMARY_MODEL,\n",
        "        \"prompt\": \"What is 2+2? Answer in one word.\",\n",
        "        \"stream\": False\n",
        "    }).encode()\n",
        "    req = urllib.request.Request(\n",
        "        \"http://localhost:11434/api/generate\",\n",
        "        data=data,\n",
        "        headers={\"Content-Type\": \"application/json\"}\n",
        "    )\n",
        "    resp = urllib.request.urlopen(req, timeout=120)\n",
        "    result = json.loads(resp.read().decode())\n",
        "    elapsed = time.time() - t0\n",
        "    passed += 1\n",
        "    print(f\"OK ({elapsed:.1f}s)\")\n",
        "    print(f\"    Response: {result.get('response', '???')[:100].strip()}\")\n",
        "except Exception as e:\n",
        "    print(f\"FAIL — {str(e)[:80]}\")\n",
        "\n",
        "print(f\"\\n  Results: {passed}/{total} passed\")\n",
        "if passed >= total - 1: # Allow 1 minor failure (like 404 on health)\n",
        "    print(\"  ALL SYSTEMS GO!\")\n",
        "else:\n",
        "    print(\"  System operational but some checks failed.\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc99dfa4",
        "outputId": "11ddee6d-2dfa-4eff-c12a-d8bfd0360bc5"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "print(\"=== CHECKING SERVICE LOGS FOR ERRORS ===\")\n",
        "services = [\n",
        "    \"/tmp/orchestrator.log\",\n",
        "    \"/tmp/memory_service.log\",\n",
        "    \"/tmp/code_exec.log\",\n",
        "    \"/tmp/mcp_gateway.log\"\n",
        "]\n",
        "\n",
        "for log in services:\n",
        "    print(f\"\\n--- {log} ---\")\n",
        "    # Check if file exists first\n",
        "    try:\n",
        "        # Print last 30 lines of the log\n",
        "        result = subprocess.run([\"tail\", \"-n\", \"30\", log], capture_output=True, text=True)\n",
        "        print(result.stdout)\n",
        "        if result.stderr:\n",
        "            print(\"STDERR:\", result.stderr)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not read log: {e}\")\n"
      ],
      "id": "bc99dfa4",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CHECKING SERVICE LOGS FOR ERRORS ===\n",
            "\n",
            "--- /tmp/orchestrator.log ---\n",
            "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/content/ai_final/agentic-framework-main/orchestrator/__init__.py\", line 19, in <module>\n",
            "    from .service.config import OrchestratorConfig, config\n",
            "  File \"/content/ai_final/agentic-framework-main/orchestrator/service/__init__.py\", line 7, in <module>\n",
            "    from .config import OrchestratorConfig, config\n",
            "  File \"/content/ai_final/agentic-framework-main/orchestrator/service/config.py\", line 184, in <module>\n",
            "    config = OrchestratorConfig()\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pydantic_settings/main.py\", line 194, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pydantic/main.py\", line 250, in __init__\n",
            "    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "pydantic_core._pydantic_core.ValidationError: 1 validation error for OrchestratorConfig\n",
            "default_llm_provider\n",
            "  Value error, default_llm_provider must be one of {'local', 'openai', 'azure', 'openclaw', 'anthropic'} [type=value_error, input_value='ollama', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
            "\n",
            "\n",
            "--- /tmp/memory_service.log ---\n",
            "    region = self._get_region(bucket_name)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/minio/api.py\", line 498, in _get_region\n",
            "    response = self._url_open(\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/minio/api.py\", line 306, in _url_open\n",
            "    response = self._http.urlopen(\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/poolmanager.py\", line 459, in urlopen\n",
            "    response = conn.urlopen(method, u.request_uri, **kw)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\", line 871, in urlopen\n",
            "    return self.urlopen(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\", line 871, in urlopen\n",
            "    return self.urlopen(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\", line 871, in urlopen\n",
            "    return self.urlopen(\n",
            "           ^^^^^^^^^^^^^\n",
            "  [Previous line repeated 2 more times]\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
            "    retries = retries.increment(\n",
            "              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/util/retry.py\", line 519, in increment\n",
            "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=9000): Max retries exceeded with url: /agent-artifacts?location= (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x785ae0740470>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "\n",
            "ERROR:    Application startup failed. Exiting.\n",
            "\n",
            "\n",
            "--- /tmp/code_exec.log ---\n",
            "chroma_url\n",
            "  Extra inputs are not permitted [type=extra_forbidden, input_value='http://localhost:8001', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden\n",
            "minio_endpoint\n",
            "  Extra inputs are not permitted [type=extra_forbidden, input_value='localhost:9000', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden\n",
            "minio_access_key\n",
            "  Extra inputs are not permitted [type=extra_forbidden, input_value='minioadmin', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden\n",
            "minio_secret_key\n",
            "  Extra inputs are not permitted [type=extra_forbidden, input_value='minioadmin', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden\n",
            "jwt_secret_key\n",
            "  Extra inputs are not permitted [type=extra_forbidden, input_value='colab-dev-secret-key-change-in-production', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden\n",
            "environment\n",
            "  Extra inputs are not permitted [type=extra_forbidden, input_value='development', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden\n",
            "pythonpath\n",
            "  Extra inputs are not permitted [type=extra_forbidden, input_value='/content/ai_final/agentic-framework-main', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden\n",
            "workspace_root\n",
            "  Extra inputs are not permitted [type=extra_forbidden, input_value='/content/ai_final/agenti...ramework-main/workspace', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden\n",
            "websocket_enabled\n",
            "  Extra inputs are not permitted [type=extra_forbidden, input_value='true', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden\n",
            "index_codebase\n",
            "  Extra inputs are not permitted [type=extra_forbidden, input_value='true', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden\n",
            "\n",
            "\n",
            "--- /tmp/mcp_gateway.log ---\n",
            "/content/ai_final/agentic-framework-main/mcp_gateway/service/models.py:209: UserWarning: Field name \"schema\" in \"ToolSchemaResponse\" shadows an attribute in parent \"BaseModel\"\n",
            "  class ToolSchemaResponse(BaseModel):\n",
            "INFO:     Started server process [21138]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 8080): address already in use\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "Starting mcp-gateway v1.0.0\n",
            "Initialized sample tools in catalog\n",
            "Shutting down MCP Gateway\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "737c2ece",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "737c2ece",
        "outputId": "1d45d1b6-f75d-4173-f6f4-222fbc122ab5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "KEEP-ALIVE WATCHDOG STARTED\n",
            "  Monitoring services every 60s with auto-restart.\n",
            "  Status updates every 5 minutes.\n",
            "  Stop with: Runtime > Interrupt execution\n",
            "============================================================\n",
            "    Restarting MCP Gateway on port 8080... PID 19057\n",
            "    Restarting Memory Service on port 8002... PID 19086\n",
            "    Restarting Code Executor on port 8004... PID 19108\n",
            "    Restarting Orchestrator on port 8000... PID 19129\n",
            "    Restarting MCP Gateway on port 8080... PID 19431\n",
            "    Restarting Memory Service on port 8002... PID 19460\n",
            "    Restarting Code Executor on port 8004... PID 19485\n",
            "    Restarting Orchestrator on port 8000... PID 19511\n",
            "    Restarting MCP Gateway on port 8080... PID 19813\n",
            "    Restarting Memory Service on port 8002... PID 19840\n",
            "    Restarting Code Executor on port 8004... PID 19865\n",
            "    Restarting Orchestrator on port 8000... PID 19892\n",
            "    Restarting MCP Gateway on port 8080... PID 20194\n",
            "    Restarting Memory Service on port 8002... PID 20221\n",
            "    Restarting Code Executor on port 8004... PID 20246\n",
            "    Restarting Orchestrator on port 8000... PID 20273\n",
            "    Restarting MCP Gateway on port 8080... PID 20581\n",
            "    Restarting Memory Service on port 8002... PID 20608\n",
            "    Restarting Code Executor on port 8004... PID 20635\n",
            "    Restarting Orchestrator on port 8000... PID 20662\n",
            "  [02:17:19] Services: 1/5 | Ollama: OK | Restarts this cycle: 4\n",
            "    Restarting MCP Gateway on port 8080... PID 20961\n",
            "    Restarting Memory Service on port 8002... PID 20988\n",
            "    Restarting Code Executor on port 8004... PID 21011\n",
            "    Restarting Orchestrator on port 8000... \n",
            "  Watchdog stopped by user.\n"
          ]
        }
      ],
      "source": [
        "# ╔══════════════════════════════════════════════════════════════╗\n",
        "# ║  PHASE 7: Keep-Alive (prevents Colab from disconnecting)    ║\n",
        "# ╚══════════════════════════════════════════════════════════════╝\n",
        "#\n",
        "# This cell runs a background loop that:\n",
        "#  1. Pings all services every 60 seconds\n",
        "#  2. Auto-restarts any crashed service\n",
        "#  3. Prints a status update every 5 minutes\n",
        "#  4. Keeps the Colab runtime alive\n",
        "#\n",
        "# Stop it with: Runtime > Interrupt execution (or Ctrl+M I)\n",
        "#\n",
        "import subprocess, os, sys, time, urllib.request, json, signal\n",
        "from datetime import datetime\n",
        "\n",
        "FRAMEWORK_DIR = \"/content/ai_final/agentic-framework-main\"\n",
        "\n",
        "service_defs = [\n",
        "    {\"name\": \"MCP Gateway\",      \"module\": \"mcp_gateway.service.main:app\",      \"port\": 8080, \"log\": \"/tmp/mcp_gateway.log\",      \"env\": {\"REDIS_URL\": \"redis://localhost:6379/3\"}},\n",
        "    {\"name\": \"Memory Service\",   \"module\": \"memory_service.service.main:app\",   \"port\": 8002, \"log\": \"/tmp/memory_service.log\",   \"env\": {\"REDIS_URL\": \"redis://localhost:6379/2\"}},\n",
        "    {\"name\": \"SubAgent Manager\", \"module\": \"subagent_manager.service.main:app\", \"port\": 8003, \"log\": \"/tmp/subagent_manager.log\", \"env\": {\"REDIS_URL\": \"redis://localhost:6379/1\"}},\n",
        "    {\"name\": \"Code Executor\",    \"module\": \"code_exec.service.main:app\",        \"port\": 8004, \"log\": \"/tmp/code_exec.log\",        \"env\": {\"REDIS_URL\": \"redis://localhost:6379/4\"}},\n",
        "    {\"name\": \"Orchestrator\",     \"module\": \"orchestrator.service.main:app\",     \"port\": 8000, \"log\": \"/tmp/orchestrator.log\",     \"env\": {}},\n",
        "]\n",
        "\n",
        "def is_service_alive(port):\n",
        "    try:\n",
        "        url = f\"http://localhost:{port}/health\" if port != 11434 else f\"http://localhost:{port}/api/tags\"\n",
        "        urllib.request.urlopen(url, timeout=5)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def restart_service(svc):\n",
        "    \"\"\"Restart a crashed service.\"\"\"\n",
        "    print(f\"    Restarting {svc['name']} on port {svc['port']}...\", end=\" \", flush=True)\n",
        "    svc_env = {**os.environ, **svc[\"env\"]}\n",
        "    proc = subprocess.Popen(\n",
        "        [sys.executable, \"-m\", \"uvicorn\", svc[\"module\"],\n",
        "         \"--host\", \"0.0.0.0\", \"--port\", str(svc[\"port\"])],\n",
        "        cwd=FRAMEWORK_DIR,\n",
        "        stdout=open(svc[\"log\"], \"a\"),\n",
        "        stderr=subprocess.STDOUT,\n",
        "        env=svc_env\n",
        "    )\n",
        "    time.sleep(5)\n",
        "    print(f\"PID {proc.pid}\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"KEEP-ALIVE WATCHDOG STARTED\")\n",
        "print(\"  Monitoring services every 60s with auto-restart.\")\n",
        "print(\"  Status updates every 5 minutes.\")\n",
        "print(\"  Stop with: Runtime > Interrupt execution\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "cycle = 0\n",
        "try:\n",
        "    while True:\n",
        "        cycle += 1\n",
        "        restarts = 0\n",
        "\n",
        "        # Check & auto-restart services\n",
        "        for svc in service_defs:\n",
        "            if not is_service_alive(svc[\"port\"]):\n",
        "                restart_service(svc)\n",
        "                restarts += 1\n",
        "\n",
        "        # Check Ollama\n",
        "        if not is_service_alive(11434):\n",
        "            print(\"    Restarting Ollama...\", end=\" \", flush=True)\n",
        "            subprocess.Popen(\n",
        "                [\"ollama\", \"serve\"],\n",
        "                stdout=open(\"/tmp/ollama.log\", \"a\"),\n",
        "                stderr=subprocess.STDOUT,\n",
        "                env={**os.environ, \"OLLAMA_HOST\": \"0.0.0.0:11434\"}\n",
        "            )\n",
        "            time.sleep(5)\n",
        "            print(\"OK\")\n",
        "            restarts += 1\n",
        "\n",
        "        # Status update every 5 minutes (every 5th cycle)\n",
        "        if cycle % 5 == 0:\n",
        "            now = datetime.now().strftime(\"%H:%M:%S\")\n",
        "            alive = sum(1 for s in service_defs if is_service_alive(s[\"port\"]))\n",
        "            ollama_ok = is_service_alive(11434)\n",
        "            print(f\"  [{now}] Services: {alive}/{len(service_defs)} | Ollama: {'OK' if ollama_ok else 'DOWN'} | Restarts this cycle: {restarts}\")\n",
        "\n",
        "        time.sleep(60)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n  Watchdog stopped by user.\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e199e12f",
        "outputId": "b01fefa7-23cd-464f-e26f-fbb46fc178be"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "logs = [\"/tmp/minio.log\", \"/tmp/code_exec.log\", \"/tmp/mcp_gateway.log\"]\n",
        "\n",
        "print(\"=== SERVICE LOGS ===\")\n",
        "for log in logs:\n",
        "    print(f\"\\n--- {log} ---\")\n",
        "    try:\n",
        "        # Check if file exists and has content\n",
        "        if os.path.exists(log):\n",
        "            with open(log, 'r') as f:\n",
        "                content = f.read().strip()\n",
        "                if content:\n",
        "                    print(content[-2000:]) # Print last 2000 chars\n",
        "                else:\n",
        "                    print(\"(Empty file)\")\n",
        "        else:\n",
        "            print(\"(File not found)\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {log}: {e}\")"
      ],
      "id": "e199e12f",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== SERVICE LOGS ===\n",
            "\n",
            "--- /tmp/minio.log ---\n",
            "FATAL Unable to start the server: Specified port is already in use\n",
            "      > Please ensure no other program uses the same address/port\n",
            "\n",
            "--- /tmp/code_exec.log ---\n",
            "http://localhost:8001', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden\n",
            "minio_endpoint\n",
            "  Extra inputs are not permitted [type=extra_forbidden, input_value='localhost:9000', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden\n",
            "minio_access_key\n",
            "  Extra inputs are not permitted [type=extra_forbidden, input_value='minioadmin', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden\n",
            "minio_secret_key\n",
            "  Extra inputs are not permitted [type=extra_forbidden, input_value='minioadmin', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden\n",
            "jwt_secret_key\n",
            "  Extra inputs are not permitted [type=extra_forbidden, input_value='colab-dev-secret-key-change-in-production', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden\n",
            "environment\n",
            "  Extra inputs are not permitted [type=extra_forbidden, input_value='development', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden\n",
            "pythonpath\n",
            "  Extra inputs are not permitted [type=extra_forbidden, input_value='/content/ai_final/agentic-framework-main', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden\n",
            "workspace_root\n",
            "  Extra inputs are not permitted [type=extra_forbidden, input_value='/content/ai_final/agenti...ramework-main/workspace', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden\n",
            "websocket_enabled\n",
            "  Extra inputs are not permitted [type=extra_forbidden, input_value='true', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden\n",
            "index_codebase\n",
            "  Extra inputs are not permitted [type=extra_forbidden, input_value='true', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden\n",
            "\n",
            "--- /tmp/mcp_gateway.log ---\n",
            "/content/ai_final/agentic-framework-main/mcp_gateway/service/models.py:209: UserWarning: Field name \"schema\" in \"ToolSchemaResponse\" shadows an attribute in parent \"BaseModel\"\n",
            "  class ToolSchemaResponse(BaseModel):\n",
            "INFO:     Started server process [24066]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 8080): address already in use\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "Starting mcp-gateway v1.0.0\n",
            "Initialized sample tools in catalog\n",
            "Shutting down MCP Gateway\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1799502a",
        "outputId": "52d291cf-c1e2-4253-8417-61b23c31797c"
      },
      "source": [
        "# ── Repair & Restart Services ──\n",
        "import subprocess, time, sys, os, urllib.request\n",
        "\n",
        "# Detect correct framework directory\n",
        "POSSIBLE_DIRS = [\n",
        "    \"/content/ai_final/agentic-framework-main\",\n",
        "    \"/content/ai_final\"\n",
        "]\n",
        "FRAMEWORK_DIR = \"/content/ai_final\"\n",
        "for d in POSSIBLE_DIRS:\n",
        "    if os.path.exists(d) and os.path.exists(os.path.join(d, \"orchestrator\")):\n",
        "        FRAMEWORK_DIR = d\n",
        "        break\n",
        "\n",
        "print(f\"Using Framework Directory: {FRAMEWORK_DIR}\")\n",
        "\n",
        "services = [\n",
        "    {\"name\": \"Code Executor\",    \"module\": \"code_exec.service.main:app\",        \"port\": 8004, \"log\": \"/tmp/code_exec.log\",        \"env\": {\"REDIS_URL\": \"redis://localhost:6379/4\"}},\n",
        "    {\"name\": \"Memory Service\",   \"module\": \"memory_service.service.main:app\",   \"port\": 8002, \"log\": \"/tmp/memory_service.log\",   \"env\": {\"REDIS_URL\": \"redis://localhost:6379/2\"}},\n",
        "    {\"name\": \"SubAgent Manager\", \"module\": \"subagent_manager.service.main:app\", \"port\": 8003, \"log\": \"/tmp/subagent_manager.log\", \"env\": {\"REDIS_URL\": \"redis://localhost:6379/1\"}},\n",
        "    {\"name\": \"MCP Gateway\",      \"module\": \"mcp_gateway.service.main:app\",      \"port\": 8080, \"log\": \"/tmp/mcp_gateway.log\",      \"env\": {\"REDIS_URL\": \"redis://localhost:6379/3\"}},\n",
        "    {\"name\": \"Orchestrator\",     \"module\": \"orchestrator.service.main:app\",     \"port\": 8000, \"log\": \"/tmp/orchestrator.log\",     \"env\": {}},\n",
        "]\n",
        "\n",
        "def check_port(port):\n",
        "    try:\n",
        "        urllib.request.urlopen(f\"http://localhost:{port}/health\", timeout=2)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "print(\"Stopping any stuck services...\")\n",
        "subprocess.run([\"pkill\", \"-f\", \"uvicorn\"])\n",
        "time.sleep(2)\n",
        "\n",
        "print(\"Restarting services with log inspection...\")\n",
        "service_env = {**os.environ, \"PYTHONPATH\": FRAMEWORK_DIR}\n",
        "\n",
        "for svc in services:\n",
        "    print(f\"Starting {svc['name']} (:{svc['port']})...\", end=\" \", flush=True)\n",
        "    svc_env = {**service_env, **svc[\"env\"]}\n",
        "\n",
        "    # Start process\n",
        "    subprocess.Popen(\n",
        "        [sys.executable, \"-m\", \"uvicorn\", svc[\"module\"],\n",
        "         \"--host\", \"0.0.0.0\", \"--port\", str(svc[\"port\"])],\n",
        "        cwd=FRAMEWORK_DIR,\n",
        "        stdout=open(svc[\"log\"], \"w\"),\n",
        "        stderr=subprocess.STDOUT,\n",
        "        env=svc_env\n",
        "    )\n",
        "\n",
        "    # Wait and check\n",
        "    time.sleep(4)\n",
        "    if check_port(svc[\"port\"]):\n",
        "        print(\"OK\")\n",
        "    else:\n",
        "        # Check if process is even running\n",
        "        pid_check = subprocess.run([\"pgrep\", \"-f\", f\"port {svc['port']}\"], capture_output=True)\n",
        "        if pid_check.returncode == 0:\n",
        "             print(\"Running (but health check failed - still initializing?)\")\n",
        "        else:\n",
        "             print(\"FAIL (Crashed)\")\n",
        "             print(f\"--- Last 20 lines of {svc['log']} ---\")\n",
        "             if os.path.exists(svc[\"log\"]):\n",
        "                 subprocess.run([\"tail\", \"-n\", \"20\", svc[\"log\"]])\n",
        "             else:\n",
        "                 print(\"Log file not found.\")\n",
        "             print(\"------------------------------------\")\n",
        "\n",
        "print(\"\\nRepair complete. Try running Phase 6 (Smoke Test) again.\")"
      ],
      "id": "1799502a",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Framework Directory: /content/ai_final/agentic-framework-main\n",
            "Stopping any stuck services...\n",
            "Restarting services with log inspection...\n",
            "Starting Code Executor (:8004)... FAIL (Crashed)\n",
            "--- Last 20 lines of /tmp/code_exec.log ---\n",
            "------------------------------------\n",
            "Starting Memory Service (:8002)... Running (but health check failed - still initializing?)\n",
            "Starting SubAgent Manager (:8003)... OK\n",
            "Starting MCP Gateway (:8080)... FAIL (Crashed)\n",
            "--- Last 20 lines of /tmp/mcp_gateway.log ---\n",
            "------------------------------------\n",
            "Starting Orchestrator (:8000)... FAIL (Crashed)\n",
            "--- Last 20 lines of /tmp/orchestrator.log ---\n",
            "------------------------------------\n",
            "\n",
            "Repair complete. Try running Phase 6 (Smoke Test) again.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44117a36",
      "metadata": {
        "id": "44117a36"
      },
      "source": [
        "---\n",
        "## Utility Cells (run manually as needed)\n",
        "\n",
        "The cells below are optional — run them when you want to interact with the system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4c8eb3bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c8eb3bb",
        "outputId": "70ec9381-b385-49a0-b510-ec2f6bcc0b86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task: Write a Python function that calculates the Fibonacci sequence up to n terms, with proper error handling and type hints.\n",
            "\n",
            "Error: <urlopen error [Errno 111] Connection refused>\n",
            "Tip: !tail -100 /tmp/orchestrator.log\n"
          ]
        }
      ],
      "source": [
        "# ── Send a task to the Orchestrator ──\n",
        "import json, urllib.request\n",
        "\n",
        "task = \"Write a Python function that calculates the Fibonacci sequence up to n terms, with proper error handling and type hints.\"\n",
        "\n",
        "print(f\"Task: {task}\\n\")\n",
        "data = json.dumps({\"message\": task, \"session_id\": \"colab-auto-001\"}).encode()\n",
        "req = urllib.request.Request(\n",
        "    \"http://localhost:8000/chat\",\n",
        "    data=data,\n",
        "    headers={\"Content-Type\": \"application/json\"}\n",
        ")\n",
        "try:\n",
        "    resp = urllib.request.urlopen(req, timeout=300)\n",
        "    result = json.loads(resp.read().decode())\n",
        "    print(json.dumps(result, indent=2)[:3000])\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Tip: !tail -100 /tmp/orchestrator.log\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "16fe6036",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16fe6036",
        "outputId": "86afb195-1b75-4571-c5d6-a22888db7bf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 50 lines of orchestrator:\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['tail', '-50', '/tmp/orchestrator.log'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# ── View service logs ──\n",
        "# Change SERVICE to: orchestrator, memory_service, subagent_manager,\n",
        "#                     mcp_gateway, code_exec, ollama, chroma, minio, dashboard\n",
        "SERVICE = \"orchestrator\"\n",
        "LINES = 50\n",
        "\n",
        "import subprocess\n",
        "print(f\"Last {LINES} lines of {SERVICE}:\")\n",
        "print(\"=\" * 60)\n",
        "subprocess.run([\"tail\", f\"-{LINES}\", f\"/tmp/{SERVICE}.log\"], capture_output=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "41e3d312",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41e3d312",
        "outputId": "f70395ac-aece-4184-fc5b-41d038321ac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU:\n",
            "\n",
            "RAM: 1.6/12.7 GB (15.3%)\n",
            "Disk: 57.0/235.7 GB\n",
            "\n",
            "Running services:\n",
            "  PID 21086: /usr/bin/python3 -m uvicorn memory_service.service.main:app --host 0.0.0.0 --por\n",
            "  PID 21116: /usr/bin/python3 -m uvicorn subagent_manager.service.main:app --host 0.0.0.0 --p\n"
          ]
        }
      ],
      "source": [
        "# ── System resource monitor ──\n",
        "import subprocess, psutil, shutil\n",
        "\n",
        "print(\"GPU:\")\n",
        "subprocess.run(\"nvidia-smi\", shell=True)\n",
        "\n",
        "mem = psutil.virtual_memory()\n",
        "print(f\"\\nRAM: {mem.used/1024**3:.1f}/{mem.total/1024**3:.1f} GB ({mem.percent}%)\")\n",
        "\n",
        "disk = shutil.disk_usage(\"/\")\n",
        "print(f\"Disk: {(disk.total-disk.free)/1024**3:.1f}/{disk.total/1024**3:.1f} GB\")\n",
        "\n",
        "print(\"\\nRunning services:\")\n",
        "for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n",
        "    try:\n",
        "        cmd = \" \".join(proc.info.get('cmdline', []))\n",
        "        if 'uvicorn' in cmd or 'ollama' in proc.info.get('name', '').lower():\n",
        "            print(f\"  PID {proc.info['pid']}: {cmd[:80]}\")\n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c48e5cd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c48e5cd",
        "outputId": "12cd732a-67ba-40fa-ca70-1ad5c0b4a503"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping all services...\n",
            "  Killed PID 14732\n",
            "  Killed PID 14780\n"
          ]
        }
      ],
      "source": [
        "# ── Restart all services ──\n",
        "import psutil, time\n",
        "\n",
        "print(\"Stopping all services...\")\n",
        "for proc in psutil.process_iter(['pid', 'cmdline']):\n",
        "    try:\n",
        "        cmd = \" \".join(proc.info.get('cmdline', []))\n",
        "        if 'uvicorn' in cmd and 'service.main' in cmd:\n",
        "            proc.kill()\n",
        "            print(f\"  Killed PID {proc.info['pid']}\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "time.sleep(3)\n",
        "print(\"Done. Re-run Phase 4 cell to restart services.\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}