{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11b353e8",
   "metadata": {},
   "source": [
    "# Agentic Framework - Google Colab Deployment\n",
    "**GPU-Accelerated Multi-Agent Orchestration on Colab Pro (H100)**\n",
    "\n",
    "This notebook deploys the full Agentic Framework stack on Google Colab:\n",
    "- **GPU (H100)**: Ollama + DeepSeek R1 14B for local LLM inference\n",
    "- **CPU**: All microservices (Orchestrator, Memory, SubAgent Manager, MCP Gateway, Code Executor)\n",
    "- **Infrastructure**: PostgreSQL, Redis, ChromaDB, MinIO\n",
    "- **Access**: ngrok tunnels for external API/Dashboard access\n",
    "\n",
    "### Prerequisites\n",
    "1. Google Colab Pro account\n",
    "2. Runtime set to **GPU (H100)** via Runtime > Change runtime type\n",
    "3. Run cells in order from top to bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecd88e5",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1: Verify GPU & System Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec394210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Verify GPU availability and system info\n",
    "# ============================================================\n",
    "import subprocess, os, shutil\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SYSTEM CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check GPU\n",
    "gpu_check = subprocess.run([\"nvidia-smi\", \"--query-gpu=name,memory.total,driver_version\", \"--format=csv,noheader\"],\n",
    "                           capture_output=True, text=True)\n",
    "if gpu_check.returncode == 0:\n",
    "    gpu_info = gpu_check.stdout.strip()\n",
    "    print(f\"[OK] GPU: {gpu_info}\")\n",
    "else:\n",
    "    print(\"[WARN] No GPU detected! Go to Runtime > Change runtime type > GPU (H100)\")\n",
    "    print(\"       The framework will still work but LLM inference will be VERY slow on CPU.\")\n",
    "\n",
    "# Check RAM\n",
    "import psutil\n",
    "ram_gb = psutil.virtual_memory().total / (1024**3)\n",
    "print(f\"[OK] RAM: {ram_gb:.1f} GB\")\n",
    "\n",
    "# Check disk\n",
    "disk = shutil.disk_usage(\"/\")\n",
    "print(f\"[OK] Disk: {disk.free / (1024**3):.1f} GB free / {disk.total / (1024**3):.1f} GB total\")\n",
    "\n",
    "# Check Python\n",
    "import sys\n",
    "print(f\"[OK] Python: {sys.version}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f74850",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2: Install System Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83863fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2: Install system-level dependencies\n",
    "# PostgreSQL, Redis, MinIO, Node.js 22\n",
    "# ============================================================\n",
    "import subprocess, os\n",
    "\n",
    "def run(cmd, desc=\"\"):\n",
    "    \"\"\"Run a shell command with status output.\"\"\"\n",
    "    if desc:\n",
    "        print(f\"  Installing {desc}...\", end=\" \", flush=True)\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        if desc:\n",
    "            print(\"[OK]\")\n",
    "    else:\n",
    "        if desc:\n",
    "            print(\"[FAIL]\")\n",
    "        print(f\"    STDERR: {result.stderr[:500]}\")\n",
    "    return result\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INSTALLING SYSTEM DEPENDENCIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Update apt\n",
    "run(\"apt-get update -qq\", \"apt update\")\n",
    "\n",
    "# PostgreSQL\n",
    "run(\"apt-get install -y -qq postgresql postgresql-client > /dev/null 2>&1\", \"PostgreSQL\")\n",
    "\n",
    "# Redis\n",
    "run(\"apt-get install -y -qq redis-server > /dev/null 2>&1\", \"Redis\")\n",
    "\n",
    "# Build tools (for native Python packages)\n",
    "run(\"apt-get install -y -qq build-essential libpq-dev > /dev/null 2>&1\", \"build tools\")\n",
    "\n",
    "# Node.js 22 (for OpenClaw and dashboard)\n",
    "run(\"curl -fsSL https://deb.nodesource.com/setup_22.x | bash - > /dev/null 2>&1\", \"Node.js repo\")\n",
    "run(\"apt-get install -y -qq nodejs > /dev/null 2>&1\", \"Node.js 22\")\n",
    "node_ver = subprocess.run(\"node --version\", shell=True, capture_output=True, text=True)\n",
    "print(f\"  Node.js version: {node_ver.stdout.strip()}\")\n",
    "\n",
    "# MinIO (S3-compatible object storage)\n",
    "run(\"wget -q https://dl.min.io/server/minio/release/linux-amd64/minio -O /usr/local/bin/minio && chmod +x /usr/local/bin/minio\",\n",
    "    \"MinIO\")\n",
    "\n",
    "print(\"\\n[OK] All system dependencies installed\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c06414",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3: Install Ollama + Pull DeepSeek R1 (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e81d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3: Install Ollama and pull DeepSeek R1 14B model\n",
    "# This uses the H100 GPU for fast inference\n",
    "# Model download is ~8GB, may take 2-5 minutes\n",
    "# ============================================================\n",
    "import subprocess, time, os\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INSTALLING OLLAMA + DEEPSEEK R1 (GPU-ACCELERATED)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Install Ollama\n",
    "print(\"  Installing Ollama...\", end=\" \", flush=True)\n",
    "result = subprocess.run(\"curl -fsSL https://ollama.com/install.sh | sh\",\n",
    "                        shell=True, capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(\"[OK]\")\n",
    "else:\n",
    "    print(f\"[FAIL] {result.stderr[:300]}\")\n",
    "\n",
    "# Start Ollama server in background (binds to all interfaces)\n",
    "print(\"  Starting Ollama server...\", end=\" \", flush=True)\n",
    "os.environ[\"OLLAMA_HOST\"] = \"0.0.0.0:11434\"\n",
    "subprocess.Popen(\n",
    "    [\"ollama\", \"serve\"],\n",
    "    stdout=open(\"/tmp/ollama.log\", \"w\"),\n",
    "    stderr=subprocess.STDOUT,\n",
    "    env={**os.environ, \"OLLAMA_HOST\": \"0.0.0.0:11434\"}\n",
    ")\n",
    "time.sleep(5)\n",
    "print(\"[OK]\")\n",
    "\n",
    "# Pull DeepSeek R1 14B model\n",
    "print(\"  Pulling deepseek-r1:14b model (this may take 2-5 min)...\")\n",
    "pull_result = subprocess.run(\n",
    "    [\"ollama\", \"pull\", \"deepseek-r1:14b\"],\n",
    "    capture_output=False, text=True\n",
    ")\n",
    "\n",
    "# Also pull the lightweight fallback model\n",
    "print(\"  Pulling llama3.2:3b fallback model...\")\n",
    "subprocess.run([\"ollama\", \"pull\", \"llama3.2:3b\"], capture_output=False, text=True)\n",
    "\n",
    "# Verify\n",
    "print(\"\\n  Available models:\")\n",
    "subprocess.run([\"ollama\", \"list\"], capture_output=False, text=True)\n",
    "\n",
    "# Verify GPU is being used\n",
    "print(\"\\n  GPU utilization:\")\n",
    "subprocess.run([\"nvidia-smi\", \"--query-gpu=utilization.gpu,memory.used,memory.total\",\n",
    "                \"--format=csv,noheader\"], capture_output=False, text=True)\n",
    "\n",
    "print(\"\\n[OK] Ollama ready with GPU acceleration\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e22df4",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4: Clone Repository & Install Python Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c9cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4: Clone the repo and install Python packages\n",
    "# ============================================================\n",
    "import subprocess, os, sys\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CLONING REPO & INSTALLING PYTHON DEPS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "REPO_URL = \"https://github.com/landonking-gif/ai_final.git\"\n",
    "INSTALL_DIR = \"/content/ai_final\"\n",
    "FRAMEWORK_DIR = f\"{INSTALL_DIR}/agentic-framework-main\"\n",
    "\n",
    "# Clone the repository\n",
    "if os.path.exists(INSTALL_DIR):\n",
    "    print(\"  Repo already exists, pulling latest...\")\n",
    "    subprocess.run([\"git\", \"-C\", INSTALL_DIR, \"pull\"], capture_output=False, text=True)\n",
    "else:\n",
    "    print(f\"  Cloning {REPO_URL}...\")\n",
    "    subprocess.run([\"git\", \"clone\", REPO_URL, INSTALL_DIR], capture_output=False, text=True)\n",
    "\n",
    "# Create symlinks so Python imports work (hyphen -> underscore)\n",
    "os.chdir(FRAMEWORK_DIR)\n",
    "symlinks = {\n",
    "    \"memory_service\": \"memory-service\",\n",
    "    \"subagent_manager\": \"subagent-manager\",\n",
    "    \"mcp_gateway\": \"mcp-gateway\",\n",
    "    \"code_exec\": \"code-exec\",\n",
    "}\n",
    "for link_name, target in symlinks.items():\n",
    "    if not os.path.exists(link_name) and os.path.exists(target):\n",
    "        os.symlink(target, link_name)\n",
    "        print(f\"  Created symlink: {link_name} -> {target}\")\n",
    "\n",
    "# Install Python dependencies\n",
    "print(\"\\n  Installing Python packages (this may take 2-3 min)...\")\n",
    "subprocess.run(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "     \"-r\", f\"{FRAMEWORK_DIR}/requirements.txt\"],\n",
    "    capture_output=False, text=True\n",
    ")\n",
    "\n",
    "# Install additional packages needed for Colab\n",
    "subprocess.run(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "     \"pyngrok\", \"asyncpg\", \"aiofiles\"],\n",
    "    capture_output=False, text=True\n",
    ")\n",
    "\n",
    "# Install OpenClaw globally\n",
    "print(\"  Installing OpenClaw...\")\n",
    "subprocess.run([\"npm\", \"install\", \"-g\", \"openclaw@latest\"],\n",
    "               capture_output=True, text=True)\n",
    "\n",
    "# Add framework dir to Python path\n",
    "if FRAMEWORK_DIR not in sys.path:\n",
    "    sys.path.insert(0, FRAMEWORK_DIR)\n",
    "os.environ[\"PYTHONPATH\"] = FRAMEWORK_DIR\n",
    "\n",
    "print(f\"\\n[OK] Repository cloned to {INSTALL_DIR}\")\n",
    "print(f\"[OK] Framework at {FRAMEWORK_DIR}\")\n",
    "print(\"[OK] All Python dependencies installed\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77644124",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5: Start Infrastructure Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782b184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 5: Start PostgreSQL, Redis, ChromaDB, MinIO\n",
    "# ============================================================\n",
    "import subprocess, time, os\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING INFRASTRUCTURE SERVICES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- PostgreSQL ---\n",
    "print(\"  Starting PostgreSQL...\", end=\" \", flush=True)\n",
    "subprocess.run(\"service postgresql start\", shell=True, capture_output=True)\n",
    "time.sleep(2)\n",
    "\n",
    "# Create database and user\n",
    "pg_cmds = [\n",
    "    \"CREATE USER agent_user WITH PASSWORD 'agent_pass' CREATEDB;\",\n",
    "    \"CREATE DATABASE agentic_framework OWNER agent_user;\",\n",
    "    \"GRANT ALL PRIVILEGES ON DATABASE agentic_framework TO agent_user;\",\n",
    "]\n",
    "for cmd in pg_cmds:\n",
    "    subprocess.run(\n",
    "        [\"sudo\", \"-u\", \"postgres\", \"psql\", \"-c\", cmd],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "# Verify\n",
    "pg_check = subprocess.run(\n",
    "    [\"sudo\", \"-u\", \"postgres\", \"psql\", \"-c\", \"SELECT 1;\"],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "print(\"[OK]\" if pg_check.returncode == 0 else \"[FAIL]\")\n",
    "\n",
    "# --- Redis ---\n",
    "print(\"  Starting Redis...\", end=\" \", flush=True)\n",
    "subprocess.run(\"redis-server --daemonize yes --port 6379\", shell=True, capture_output=True)\n",
    "time.sleep(1)\n",
    "redis_check = subprocess.run(\"redis-cli ping\", shell=True, capture_output=True, text=True)\n",
    "print(\"[OK]\" if \"PONG\" in redis_check.stdout else \"[FAIL]\")\n",
    "\n",
    "# --- ChromaDB ---\n",
    "print(\"  Starting ChromaDB (vector store)...\", end=\" \", flush=True)\n",
    "os.makedirs(\"/tmp/chroma_data\", exist_ok=True)\n",
    "subprocess.Popen(\n",
    "    [\"chroma\", \"run\", \"--host\", \"0.0.0.0\", \"--port\", \"8001\",\n",
    "     \"--path\", \"/tmp/chroma_data\"],\n",
    "    stdout=open(\"/tmp/chroma.log\", \"w\"),\n",
    "    stderr=subprocess.STDOUT\n",
    ")\n",
    "time.sleep(3)\n",
    "print(\"[OK]\")\n",
    "\n",
    "# --- MinIO ---\n",
    "print(\"  Starting MinIO (S3 storage)...\", end=\" \", flush=True)\n",
    "os.makedirs(\"/tmp/minio_data\", exist_ok=True)\n",
    "subprocess.Popen(\n",
    "    [\"/usr/local/bin/minio\", \"server\", \"/tmp/minio_data\",\n",
    "     \"--address\", \":9000\", \"--console-address\", \":9001\"],\n",
    "    stdout=open(\"/tmp/minio.log\", \"w\"),\n",
    "    stderr=subprocess.STDOUT,\n",
    "    env={**os.environ,\n",
    "         \"MINIO_ROOT_USER\": \"minioadmin\",\n",
    "         \"MINIO_ROOT_PASSWORD\": \"minioadmin\"}\n",
    ")\n",
    "time.sleep(2)\n",
    "print(\"[OK]\")\n",
    "\n",
    "print(\"\\n  Infrastructure Summary:\")\n",
    "print(\"    PostgreSQL : localhost:5432 (agent_user/agent_pass)\")\n",
    "print(\"    Redis      : localhost:6379\")\n",
    "print(\"    ChromaDB   : localhost:8001\")\n",
    "print(\"    MinIO      : localhost:9000 (minioadmin/minioadmin)\")\n",
    "print(\"    Ollama     : localhost:11434 (GPU-accelerated)\")\n",
    "print(\"\\n[OK] All infrastructure services running\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35893c8e",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6: Configure Environment & Start Framework Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262fe491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 6: Set environment variables and start all 5 services\n",
    "# ============================================================\n",
    "import subprocess, os, sys, time\n",
    "\n",
    "FRAMEWORK_DIR = \"/content/ai_final/agentic-framework-main\"\n",
    "os.chdir(FRAMEWORK_DIR)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFIGURING & STARTING FRAMEWORK SERVICES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ---- Environment Variables ----\n",
    "env_vars = {\n",
    "    # Database\n",
    "    \"POSTGRES_URL\": \"postgresql://agent_user:agent_pass@localhost:5432/agentic_framework\",\n",
    "    \"REDIS_URL\": \"redis://localhost:6379/0\",\n",
    "    # Service URLs (all localhost since running natively)\n",
    "    \"MCP_GATEWAY_URL\": \"http://localhost:8080\",\n",
    "    \"MEMORY_SERVICE_URL\": \"http://localhost:8002\",\n",
    "    \"SUBAGENT_MANAGER_URL\": \"http://localhost:8003\",\n",
    "    \"CODE_EXECUTOR_URL\": \"http://localhost:8004\",\n",
    "    # LLM - Local GPU inference\n",
    "    \"OLLAMA_ENDPOINT\": \"http://localhost:11434\",\n",
    "    \"OLLAMA_BASE_URL\": \"http://localhost:11434\",\n",
    "    \"LOCAL_MODEL\": \"deepseek-r1:14b\",\n",
    "    \"FALLBACK_MODEL\": \"llama3.2:3b\",\n",
    "    \"DEFAULT_LLM_PROVIDER\": \"ollama\",\n",
    "    \"LLM_PROVIDER\": \"ollama\",\n",
    "    # OpenClaw (disabled - using Ollama directly for simplicity)\n",
    "    \"USE_OPENCLAW\": \"false\",\n",
    "    # Storage\n",
    "    \"CHROMA_URL\": \"http://localhost:8001\",\n",
    "    \"MINIO_ENDPOINT\": \"localhost:9000\",\n",
    "    \"MINIO_ACCESS_KEY\": \"minioadmin\",\n",
    "    \"MINIO_SECRET_KEY\": \"minioadmin\",\n",
    "    # Security\n",
    "    \"JWT_SECRET_KEY\": \"colab-dev-secret-key-change-in-production\",\n",
    "    # General\n",
    "    \"ENVIRONMENT\": \"development\",\n",
    "    \"PYTHONPATH\": FRAMEWORK_DIR,\n",
    "    \"WORKSPACE_ROOT\": f\"{FRAMEWORK_DIR}/workspace\",\n",
    "    \"WEBSOCKET_ENABLED\": \"true\",\n",
    "    \"INDEX_CODEBASE\": \"true\",\n",
    "}\n",
    "\n",
    "# Apply all env vars\n",
    "for key, value in env_vars.items():\n",
    "    os.environ[key] = value\n",
    "\n",
    "# Write .env file\n",
    "with open(f\"{FRAMEWORK_DIR}/.env\", \"w\") as f:\n",
    "    for key, value in env_vars.items():\n",
    "        f.write(f\"{key}={value}\\n\")\n",
    "print(\"  [OK] Environment configured (.env written)\")\n",
    "\n",
    "# Create workspace directories\n",
    "os.makedirs(f\"{FRAMEWORK_DIR}/workspace/.copilot/memory/diary\", exist_ok=True)\n",
    "os.makedirs(f\"{FRAMEWORK_DIR}/workspace/.copilot/memory/reflections\", exist_ok=True)\n",
    "os.makedirs(f\"{FRAMEWORK_DIR}/workspace/ralph-work\", exist_ok=True)\n",
    "\n",
    "# ---- Start Services ----\n",
    "service_env = {**os.environ}\n",
    "\n",
    "services = [\n",
    "    {\n",
    "        \"name\": \"MCP Gateway\",\n",
    "        \"cmd\": [sys.executable, \"-m\", \"uvicorn\",\n",
    "                \"mcp_gateway.service.main:app\",\n",
    "                \"--host\", \"0.0.0.0\", \"--port\", \"8080\"],\n",
    "        \"port\": 8080,\n",
    "        \"log\": \"/tmp/mcp_gateway.log\",\n",
    "        \"env_extra\": {\"REDIS_URL\": \"redis://localhost:6379/3\"},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Memory Service\",\n",
    "        \"cmd\": [sys.executable, \"-m\", \"uvicorn\",\n",
    "                \"memory_service.service.main:app\",\n",
    "                \"--host\", \"0.0.0.0\", \"--port\", \"8002\"],\n",
    "        \"port\": 8002,\n",
    "        \"log\": \"/tmp/memory_service.log\",\n",
    "        \"env_extra\": {\"REDIS_URL\": \"redis://localhost:6379/2\"},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"SubAgent Manager\",\n",
    "        \"cmd\": [sys.executable, \"-m\", \"uvicorn\",\n",
    "                \"subagent_manager.service.main:app\",\n",
    "                \"--host\", \"0.0.0.0\", \"--port\", \"8003\"],\n",
    "        \"port\": 8003,\n",
    "        \"log\": \"/tmp/subagent_manager.log\",\n",
    "        \"env_extra\": {\n",
    "            \"REDIS_URL\": \"redis://localhost:6379/1\",\n",
    "            \"SUBAGENT_USE_OPENCLAW\": \"false\",\n",
    "            \"SUBAGENT_LLM_PROVIDER\": \"ollama\",\n",
    "            \"SUBAGENT_LLM_MODEL\": \"deepseek-r1:14b\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Code Executor\",\n",
    "        \"cmd\": [sys.executable, \"-m\", \"uvicorn\",\n",
    "                \"code_exec.service.main:app\",\n",
    "                \"--host\", \"0.0.0.0\", \"--port\", \"8004\"],\n",
    "        \"port\": 8004,\n",
    "        \"log\": \"/tmp/code_exec.log\",\n",
    "        \"env_extra\": {\"REDIS_URL\": \"redis://localhost:6379/4\"},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Orchestrator\",\n",
    "        \"cmd\": [sys.executable, \"-m\", \"uvicorn\",\n",
    "                \"orchestrator.service.main:app\",\n",
    "                \"--host\", \"0.0.0.0\", \"--port\", \"8000\"],\n",
    "        \"port\": 8000,\n",
    "        \"log\": \"/tmp/orchestrator.log\",\n",
    "        \"env_extra\": {},\n",
    "    },\n",
    "]\n",
    "\n",
    "started_pids = {}\n",
    "\n",
    "for svc in services:\n",
    "    print(f\"  Starting {svc['name']} (port {svc['port']})...\", end=\" \", flush=True)\n",
    "    svc_env = {**service_env, **svc.get(\"env_extra\", {})}\n",
    "    proc = subprocess.Popen(\n",
    "        svc[\"cmd\"],\n",
    "        cwd=FRAMEWORK_DIR,\n",
    "        stdout=open(svc[\"log\"], \"w\"),\n",
    "        stderr=subprocess.STDOUT,\n",
    "        env=svc_env\n",
    "    )\n",
    "    started_pids[svc[\"name\"]] = proc.pid\n",
    "    time.sleep(3)  # Give each service time to start\n",
    "    print(f\"[OK] (PID {proc.pid})\")\n",
    "\n",
    "# Wait for services to fully initialize\n",
    "print(\"\\n  Waiting for services to initialize (15s)...\")\n",
    "time.sleep(15)\n",
    "\n",
    "# ---- Health Checks ----\n",
    "import urllib.request\n",
    "\n",
    "print(\"\\n  Health Checks:\")\n",
    "health_endpoints = [\n",
    "    (\"Orchestrator\", \"http://localhost:8000/health\"),\n",
    "    (\"Memory Service\", \"http://localhost:8002/health\"),\n",
    "    (\"SubAgent Manager\", \"http://localhost:8003/health\"),\n",
    "    (\"MCP Gateway\", \"http://localhost:8080/health\"),\n",
    "    (\"Code Executor\", \"http://localhost:8004/health\"),\n",
    "    (\"Ollama\", \"http://localhost:11434/api/tags\"),\n",
    "]\n",
    "\n",
    "for name, url in health_endpoints:\n",
    "    try:\n",
    "        req = urllib.request.urlopen(url, timeout=5)\n",
    "        status = req.getcode()\n",
    "        print(f\"    {name:20s} : [OK] ({status})\")\n",
    "    except Exception as e:\n",
    "        print(f\"    {name:20s} : [STARTING] ({str(e)[:50]})\")\n",
    "        print(f\"      -> Check log: tail -50 {[s for s in services if s['name']==name][0]['log'] if any(s['name']==name for s in services) else 'N/A'}\")\n",
    "\n",
    "print(\"\\n[OK] Framework services started\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfa64aa",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7: Set Up ngrok Tunnels for External Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f782d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7: Create ngrok tunnels for external access\n",
    "# You get temporary public URLs to access the services\n",
    "# ============================================================\n",
    "from pyngrok import ngrok, conf\n",
    "import os\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SETTING UP EXTERNAL ACCESS (ngrok tunnels)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Optional: Set your ngrok auth token for longer sessions\n",
    "# Get a free token at https://dashboard.ngrok.com/signup\n",
    "# Uncomment and set your token:\n",
    "# NGROK_AUTH_TOKEN = \"your-ngrok-auth-token-here\"\n",
    "# ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "\n",
    "# Create tunnel for Orchestrator API (main entry point)\n",
    "print(\"  Creating tunnel for Orchestrator API (port 8000)...\")\n",
    "api_tunnel = ngrok.connect(8000, \"http\")\n",
    "api_url = api_tunnel.public_url\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ACCESS POINTS (use these URLs from any browser):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\")\n",
    "print(f\"  API Base URL:    {api_url}\")\n",
    "print(f\"  API Docs:        {api_url}/docs\")\n",
    "print(f\"  Health Check:    {api_url}/health\")\n",
    "print(f\"  WebSocket:       {api_url.replace('http', 'ws')}/ws\")\n",
    "print(f\"\")\n",
    "print(f\"  Local-only endpoints (inside Colab):\")\n",
    "print(f\"    Orchestrator:  http://localhost:8000\")\n",
    "print(f\"    Memory:        http://localhost:8002\")\n",
    "print(f\"    SubAgents:     http://localhost:8003\")\n",
    "print(f\"    MCP Gateway:   http://localhost:8080\")\n",
    "print(f\"    Code Exec:     http://localhost:8004\")\n",
    "print(f\"    Ollama:        http://localhost:11434\")\n",
    "print(f\"\")\n",
    "print(\"NOTE: ngrok URLs are temporary and change each session.\")\n",
    "print(\"      For persistent URLs, set NGROK_AUTH_TOKEN above.\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store for later use\n",
    "os.environ[\"COLAB_API_URL\"] = api_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626acb42",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8: Test the Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b7e9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 8: Run tests to verify everything works\n",
    "# ============================================================\n",
    "import json, urllib.request, subprocess\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TESTING DEPLOYMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tests_passed = 0\n",
    "tests_total = 0\n",
    "\n",
    "def test(name, url, expected_status=200):\n",
    "    global tests_passed, tests_total\n",
    "    tests_total += 1\n",
    "    try:\n",
    "        req = urllib.request.urlopen(url, timeout=10)\n",
    "        status = req.getcode()\n",
    "        body = req.read().decode()[:200]\n",
    "        if status == expected_status:\n",
    "            tests_passed += 1\n",
    "            print(f\"  [PASS] {name} -> {status}\")\n",
    "        else:\n",
    "            print(f\"  [FAIL] {name} -> {status} (expected {expected_status})\")\n",
    "        return body\n",
    "    except Exception as e:\n",
    "        print(f\"  [FAIL] {name} -> {str(e)[:80]}\")\n",
    "        return None\n",
    "\n",
    "# Test 1: Orchestrator health\n",
    "test(\"Orchestrator /health\", \"http://localhost:8000/health\")\n",
    "\n",
    "# Test 2: API docs\n",
    "test(\"Orchestrator /docs\", \"http://localhost:8000/docs\")\n",
    "\n",
    "# Test 3: Memory service\n",
    "test(\"Memory Service /health\", \"http://localhost:8002/health\")\n",
    "\n",
    "# Test 4: MCP Gateway\n",
    "test(\"MCP Gateway /health\", \"http://localhost:8080/health\")\n",
    "\n",
    "# Test 5: SubAgent Manager\n",
    "test(\"SubAgent Manager /health\", \"http://localhost:8003/health\")\n",
    "\n",
    "# Test 6: Code Executor\n",
    "test(\"Code Executor /health\", \"http://localhost:8004/health\")\n",
    "\n",
    "# Test 7: Ollama API\n",
    "test(\"Ollama API /api/tags\", \"http://localhost:11434/api/tags\")\n",
    "\n",
    "# Test 8: Ollama inference (GPU test)\n",
    "print(\"\\n  Testing LLM inference (GPU)...\")\n",
    "tests_total += 1\n",
    "try:\n",
    "    import time\n",
    "    start = time.time()\n",
    "    data = json.dumps({\n",
    "        \"model\": \"deepseek-r1:14b\",\n",
    "        \"prompt\": \"What is 2+2? Answer in one word.\",\n",
    "        \"stream\": False\n",
    "    }).encode()\n",
    "    req = urllib.request.Request(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        data=data,\n",
    "        headers={\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "    resp = urllib.request.urlopen(req, timeout=120)\n",
    "    result = json.loads(resp.read().decode())\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"  [PASS] DeepSeek R1 inference -> {elapsed:.1f}s\")\n",
    "    print(f\"         Response: {result.get('response', '???')[:100]}\")\n",
    "    tests_passed += 1\n",
    "except Exception as e:\n",
    "    print(f\"  [FAIL] DeepSeek R1 inference -> {str(e)[:100]}\")\n",
    "\n",
    "# GPU utilization\n",
    "print(\"\\n  GPU Status:\")\n",
    "subprocess.run([\"nvidia-smi\", \"--query-gpu=name,utilization.gpu,memory.used,memory.total\",\n",
    "                \"--format=csv,noheader\"], capture_output=False)\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"RESULTS: {tests_passed}/{tests_total} tests passed\")\n",
    "if tests_passed == tests_total:\n",
    "    print(\"ALL SYSTEMS OPERATIONAL!\")\n",
    "else:\n",
    "    print(\"Some services may still be starting. Re-run this cell in 30s.\")\n",
    "print(f\"{'=' * 60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d2b62",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 9: Send a Task to the Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc8c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 9: Send a task to the multi-agent orchestrator\n",
    "# Edit the 'task' variable below to change what the agents do\n",
    "# ============================================================\n",
    "import json, urllib.request\n",
    "\n",
    "# ---- EDIT THIS ----\n",
    "task = \"Write a Python function that calculates the Fibonacci sequence up to n terms, with proper error handling and type hints.\"\n",
    "# -------------------\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SENDING TASK TO ORCHESTRATOR\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Task: {task}\")\n",
    "print(\"\")\n",
    "\n",
    "data = json.dumps({\n",
    "    \"message\": task,\n",
    "    \"session_id\": \"colab-session-001\"\n",
    "}).encode()\n",
    "\n",
    "req = urllib.request.Request(\n",
    "    \"http://localhost:8000/chat\",\n",
    "    data=data,\n",
    "    headers={\"Content-Type\": \"application/json\"}\n",
    ")\n",
    "\n",
    "try:\n",
    "    resp = urllib.request.urlopen(req, timeout=300)  # 5 min timeout for complex tasks\n",
    "    result = json.loads(resp.read().decode())\n",
    "    print(\"Response:\")\n",
    "    print(\"-\" * 60)\n",
    "    if isinstance(result, dict):\n",
    "        print(json.dumps(result, indent=2)[:3000])\n",
    "    else:\n",
    "        print(str(result)[:3000])\n",
    "    print(\"-\" * 60)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nTip: Check orchestrator logs with:\")\n",
    "    print(\"  !tail -100 /tmp/orchestrator.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc336c39",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 10: View Service Logs (Debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac8b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 10: View logs from any service (for debugging)\n",
    "# Change SERVICE below to see different logs\n",
    "# ============================================================\n",
    "\n",
    "# Options: \"orchestrator\", \"memory_service\", \"subagent_manager\",\n",
    "#          \"mcp_gateway\", \"code_exec\", \"ollama\", \"chroma\", \"minio\"\n",
    "SERVICE = \"orchestrator\"\n",
    "LINES = 50  # Number of lines to show\n",
    "\n",
    "import subprocess\n",
    "\n",
    "log_file = f\"/tmp/{SERVICE}.log\"\n",
    "print(f\"Last {LINES} lines of {SERVICE} log:\")\n",
    "print(\"=\" * 60)\n",
    "subprocess.run([\"tail\", f\"-{LINES}\", log_file], capture_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09b6f1c",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 11: Monitor GPU Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5f59fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 11: Monitor GPU usage and system resources\n",
    "# ============================================================\n",
    "import subprocess, psutil\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SYSTEM RESOURCE MONITOR\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# GPU\n",
    "print(\"\\nGPU:\")\n",
    "subprocess.run(\"nvidia-smi\", shell=True, capture_output=False)\n",
    "\n",
    "# CPU & RAM\n",
    "print(f\"\\nCPU Usage: {psutil.cpu_percent()}%\")\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"RAM: {mem.used/1024**3:.1f} GB / {mem.total/1024**3:.1f} GB ({mem.percent}%)\")\n",
    "\n",
    "# Disk\n",
    "import shutil\n",
    "disk = shutil.disk_usage(\"/\")\n",
    "print(f\"Disk: {(disk.total-disk.free)/1024**3:.1f} GB / {disk.total/1024**3:.1f} GB\")\n",
    "\n",
    "# Running services\n",
    "print(\"\\nRunning Python Services:\")\n",
    "for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n",
    "    try:\n",
    "        cmdline = \" \".join(proc.info.get('cmdline', []))\n",
    "        if 'uvicorn' in cmdline:\n",
    "            print(f\"  PID {proc.info['pid']}: {cmdline[:80]}\")\n",
    "    except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
    "        pass\n",
    "\n",
    "# Ollama process\n",
    "for proc in psutil.process_iter(['pid', 'name']):\n",
    "    try:\n",
    "        if 'ollama' in proc.info.get('name', '').lower():\n",
    "            print(f\"  PID {proc.info['pid']}: ollama (GPU LLM server)\")\n",
    "    except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558aa9ee",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 12: Restart Services (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d97494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 12: Restart all services (run if services crash or need reset)\n",
    "# ============================================================\n",
    "import subprocess, signal, os, psutil\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RESTARTING ALL SERVICES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Kill existing uvicorn processes\n",
    "print(\"  Stopping existing services...\")\n",
    "for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n",
    "    try:\n",
    "        cmdline = \" \".join(proc.info.get('cmdline', []))\n",
    "        if 'uvicorn' in cmdline and ('service.main' in cmdline):\n",
    "            proc.kill()\n",
    "            print(f\"    Killed PID {proc.info['pid']}\")\n",
    "    except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
    "        pass\n",
    "\n",
    "import time\n",
    "time.sleep(3)\n",
    "\n",
    "print(\"  Services stopped. Run Cell 6 to restart them.\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
